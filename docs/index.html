<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Biviana M. Su√°rez Sierra">

<title>Inferencia Estad√≠stica</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./index.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">INTRODUCCI√ìN</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Inferencia Estad√≠stica</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">INTRODUCCI√ìN</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introducci√≥n" id="toc-introducci√≥n" class="nav-link active" data-scroll-target="#introducci√≥n"><span class="header-section-number">1</span> INTRODUCCI√ìN</a>
  <ul class="collapse">
  <li><a href="#c√≥mo-navegar-este-libro" id="toc-c√≥mo-navegar-este-libro" class="nav-link" data-scroll-target="#c√≥mo-navegar-este-libro"><span class="header-section-number">1.1</span> ¬øC√≥mo navegar este libro?</a></li>
  <li><a href="#bienvenidao" id="toc-bienvenidao" class="nav-link" data-scroll-target="#bienvenidao"><span class="header-section-number">1.2</span> ¬°Bienvenida/o!</a></li>
  </ul></li>
  <li><a href="#datos-y-modelos" id="toc-datos-y-modelos" class="nav-link" data-scroll-target="#datos-y-modelos"><span class="header-section-number">2</span> DATOS Y MODELOS</a>
  <ul class="collapse">
  <li><a href="#fen√≥meno-aleatorio-y-variable-observada" id="toc-fen√≥meno-aleatorio-y-variable-observada" class="nav-link" data-scroll-target="#fen√≥meno-aleatorio-y-variable-observada"><span class="header-section-number">2.1</span> Fen√≥meno aleatorio y variable observada</a></li>
  <li><a href="#incertidumbre-inductiva-vs.-estoc√°stica" id="toc-incertidumbre-inductiva-vs.-estoc√°stica" class="nav-link" data-scroll-target="#incertidumbre-inductiva-vs.-estoc√°stica"><span class="header-section-number">2.2</span> Incertidumbre inductiva vs.&nbsp;estoc√°stica</a></li>
  </ul></li>
  <li><a href="#variable-aleatoria" id="toc-variable-aleatoria" class="nav-link" data-scroll-target="#variable-aleatoria"><span class="header-section-number">3</span> VARIABLE ALEATORIA</a>
  <ul class="collapse">
  <li><a href="#variables-y-vectores-aleatorios" id="toc-variables-y-vectores-aleatorios" class="nav-link" data-scroll-target="#variables-y-vectores-aleatorios"><span class="header-section-number">3.1</span> Variables y vectores aleatorios</a></li>
  <li><a href="#distribuci√≥n-de-una-variable-aleatoria.-funciones-de-distribuci√≥n-de-probabilidad-y-de-densidad" id="toc-distribuci√≥n-de-una-variable-aleatoria.-funciones-de-distribuci√≥n-de-probabilidad-y-de-densidad" class="nav-link" data-scroll-target="#distribuci√≥n-de-una-variable-aleatoria.-funciones-de-distribuci√≥n-de-probabilidad-y-de-densidad"><span class="header-section-number">3.2</span> Distribuci√≥n de una variable aleatoria. Funciones de distribuci√≥n, de probabilidad y de densidad</a></li>
  <li><a href="#esperanza-y-varianza" id="toc-esperanza-y-varianza" class="nav-link" data-scroll-target="#esperanza-y-varianza"><span class="header-section-number">3.3</span> Esperanza y varianza</a></li>
  <li><a href="#funci√≥n-generadora-de-momentos" id="toc-funci√≥n-generadora-de-momentos" class="nav-link" data-scroll-target="#funci√≥n-generadora-de-momentos"><span class="header-section-number">3.4</span> Funci√≥n generadora de momentos</a></li>
  <li><a href="#muestra-aleatoria-simple" id="toc-muestra-aleatoria-simple" class="nav-link" data-scroll-target="#muestra-aleatoria-simple"><span class="header-section-number">3.5</span> Muestra aleatoria simple</a></li>
  <li><a href="#modelo-param√©trico" id="toc-modelo-param√©trico" class="nav-link" data-scroll-target="#modelo-param√©trico"><span class="header-section-number">3.6</span> Modelo param√©trico</a></li>
  <li><a href="#sumas-de-variables-aleatorias" id="toc-sumas-de-variables-aleatorias" class="nav-link" data-scroll-target="#sumas-de-variables-aleatorias"><span class="header-section-number">3.7</span> Sumas de variables aleatorias</a></li>
  <li><a href="#estad√≠sticos-definidos-a-partir-de-sumas-de-variables-aleatorias" id="toc-estad√≠sticos-definidos-a-partir-de-sumas-de-variables-aleatorias" class="nav-link" data-scroll-target="#estad√≠sticos-definidos-a-partir-de-sumas-de-variables-aleatorias"><span class="header-section-number">3.8</span> Estad√≠sticos definidos a partir de sumas de variables aleatorias</a></li>
  <li><a href="#muestreo-de-una-distribuci√≥n-normal" id="toc-muestreo-de-una-distribuci√≥n-normal" class="nav-link" data-scroll-target="#muestreo-de-una-distribuci√≥n-normal"><span class="header-section-number">3.9</span> Muestreo de una distribuci√≥n normal</a>
  <ul class="collapse">
  <li><a href="#definici√≥n-de-distribuci√≥n-chi-cuadrada" id="toc-definici√≥n-de-distribuci√≥n-chi-cuadrada" class="nav-link" data-scroll-target="#definici√≥n-de-distribuci√≥n-chi-cuadrada"><span class="header-section-number">3.9.1</span> Definici√≥n de distribuci√≥n Chi cuadrada</a></li>
  <li><a href="#distribuciones-asociadas-a-la-normal" id="toc-distribuciones-asociadas-a-la-normal" class="nav-link" data-scroll-target="#distribuciones-asociadas-a-la-normal"><span class="header-section-number">3.9.2</span> Distribuciones asociadas a la normal</a></li>
  </ul></li>
  <li><a href="#referencias" id="toc-referencias" class="nav-link" data-scroll-target="#referencias"><span class="header-section-number">3.10</span> Referencias</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Inferencia Estad√≠stica</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Biviana M. Su√°rez Sierra </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="introducci√≥n" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> INTRODUCCI√ìN</h1>
<p>Este libro ha sido concebido como un recurso integral para el estudio riguroso y aplicado de la inferencia estad√≠stica. Est√° dirigido a estudiantes de programas de estad√≠stica, matem√°ticas aplicadas y disciplinas afines, y busca fortalecer la comprensi√≥n conceptual y t√©cnica de los fundamentos que sustentan el an√°lisis estad√≠stico moderno.</p>
<p>A lo largo de sus cap√≠tulos, el lector encontrar√° un desarrollo progresivo de los siguientes temas:</p>
<ul>
<li>Fundamentos de la probabilidad y los modelos estad√≠sticos.</li>
<li>Propiedades de las variables aleatorias y familias param√©tricas comunes.</li>
<li>Principios clave para la reducci√≥n de datos: suficiencia, completitud y verosimilitud.</li>
<li>Construcci√≥n y evaluaci√≥n de estimadores puntuales.</li>
<li>Estimaci√≥n por intervalos y su interpretaci√≥n inferencial.</li>
<li>Contrastes de hip√≥tesis y principios de optimalidad en pruebas estad√≠sticas.</li>
<li>Introducci√≥n a la teor√≠a de la decisi√≥n y sus aplicaciones en inferencia.</li>
</ul>
<p>El material combina el rigor formal con ejemplos y aplicaciones que ilustran c√≥mo los m√©todos estad√≠sticos permiten extraer conclusiones v√°lidas a partir de datos.</p>
<blockquote class="blockquote">
<p>Este libro se encuentra en construcci√≥n. Los cap√≠tulos se ir√°n publicando progresivamente y pueden estar sujetos a revisiones o mejoras. Por ahora, son s√≥lo trozos de contenido de otros libros o notas de clase de una selecci√≥n personal y que se ir√°n referenciando en cada cap√≠tulo.</p>
</blockquote>
<hr>
<section id="c√≥mo-navegar-este-libro" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="c√≥mo-navegar-este-libro"><span class="header-section-number">1.1</span> ¬øC√≥mo navegar este libro?</h2>
<ul>
<li>Usa el <strong>√≠ndice lateral izquierdo</strong> para acceder a cada cap√≠tulo y subcap√≠tulo.</li>
<li>Haz uso del <strong>buscador</strong> para encontrar conceptos o t√©rminos clave.</li>
<li>Revisa los apartados de ‚ÄúLista de problemas‚Äù incluidos al final de cada secci√≥n para practicar.</li>
</ul>
<hr>
</section>
<section id="bienvenidao" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="bienvenidao"><span class="header-section-number">1.2</span> ¬°Bienvenida/o!</h2>
<p>Te invito a recorrer este texto con atenci√≥n, curiosidad y sentido cr√≠tico.<br>
Espero que este libro te acompa√±e, rete y apoye en tu formaci√≥n como profesional en ciencias de datos o √°reas relacionadas.</p>
<!-- 1. [Datos y modelos](Datos%20y%20modelos.qmd)   -->
<!-- 2. [Variable aleatoria](Variable%20aleatoria.qmd)   -->
</section>
</section>
<section id="datos-y-modelos" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> DATOS Y MODELOS</h1>
<p>En esta secci√≥n establecemos la base conceptual para el an√°lisis estad√≠stico, diferenciando claramente entre el fen√≥meno aleatorio observado y la medida de probabilidad que lo describe.</p>
<section id="fen√≥meno-aleatorio-y-variable-observada" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="fen√≥meno-aleatorio-y-variable-observada"><span class="header-section-number">2.1</span> Fen√≥meno aleatorio y variable observada</h2>
<blockquote class="blockquote">
<p>‚ÄúSe observa una realizaci√≥n de un fen√≥meno aleatorio, digamos X. Este puede ser un elemento aleatorio de varios tipos: n√∫mero (variable aleatoria), un vector de dimensi√≥n finita (vector aleatorio), una funci√≥n, etc.<br>
La premisa principal es que el car√°cter aleatorio de X se concibe como una realizaci√≥n de un fen√≥meno aleatorio que tiene una distribuci√≥n de probabilidad P, donde la distribuci√≥n P es desconocida ya sea en su totalidad o en alg√∫n detalle espec√≠fico (por ejemplo, su soporte, su media, etc.). Es de inter√©s conocer P. Si la medida de probabilidad P fuese conocida, entonces no hay problema estad√≠stico propiamente, pues el problema estad√≠stico tiene que ver con <em>inferir</em> la propiedad desconocida de P con base en X.‚Äù [Ver referencia 1]</p>
</blockquote>
<ul>
<li><strong>Definici√≥n de X</strong>
<ul>
<li><strong>X</strong> puede ser un valor real <span class="math inline">\(X \in \mathbb{R}\)</span>, un vector en <span class="math inline">\(\mathbb{R}^n\)</span>, o incluso una funci√≥n <span class="math inline">\(\;X: [0,1]\to\mathbb{R}\)</span>.<br>
</li>
</ul></li>
<li><strong>Medida de probabilidad P</strong>
<ul>
<li>Desconocida: soporte, media, varianza, etc.<br>
</li>
<li>Objetivo estad√≠stico: <em>inferir</em> caracter√≠sticas de (P) a partir de la muestra (la realizaci√≥n de X).</li>
</ul></li>
</ul>
</section>
<section id="incertidumbre-inductiva-vs.-estoc√°stica" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="incertidumbre-inductiva-vs.-estoc√°stica"><span class="header-section-number">2.2</span> Incertidumbre inductiva vs.&nbsp;estoc√°stica</h2>
<blockquote class="blockquote">
<p>‚ÄúLa observaci√≥n <strong>X</strong> est√° dada, por lo que no hay incertidumbre tal como la hay en la teor√≠a de probabilidad desarrollada anteriormente en el curso. Antes, fue concebida una estructura <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span> para enfrentar el que haya incertidumbre acerca del valor de <strong>X</strong>. En el problema estad√≠stico, el valor de <strong>X</strong> ha sido observado, y la incertidumbre radica en otro punto: radica en que existe duda acerca de cu√°l <strong>P</strong> es la que produjo el valor <strong>X</strong>. En algunas ocasiones se utilizan los t√©rminos <em>incertidumbre estoc√°stica</em> e <em>incertidumbre inductiva</em> para distinguir estos dos tipos. Es com√∫n que estos se confundan entre s√≠, porque en estad√≠stica matem√°tica la teor√≠a de probabilidad constituye tambi√©n una de las maneras naturales de afrontar la cuantificaci√≥n de incertidumbre inductiva. En cualquier caso, el concebir a <strong>P</strong> como medida de probabilidad es la base para formular soluciones a la incertidumbre inductiva. Con este lenguaje, probabilidad y estad√≠stica son problemas diferentes y de cierta manera inversos. Teor√≠a de probabilidad tiene que ver con cuantificar incertidumbre acerca de <strong>X</strong> y teor√≠a estad√≠stica con cuantificar incertidumbre acerca de <strong>P</strong> a la luz de haber ya observado <strong>X</strong>.‚Äù[Ver referencia 1]</p>
</blockquote>
<ul>
<li><strong>Incertidumbre estoc√°stica</strong>: duda previa sobre el valor de X, modelada por <span class="math inline">\((\Omega,\mathcal{F},P)\)</span>.<br>
</li>
<li><strong>Incertidumbre inductiva</strong>: tras observar X, la incertidumbre se desplaza a la ley generadora P.</li>
</ul>
<hr>
<section id="ejemplos-en-matem√°tica-aplicada-e-ingenier√≠a-de-sistemas" class="level4" data-number="2.2.0.1">
<h4 data-number="2.2.0.1" class="anchored" data-anchor-id="ejemplos-en-matem√°tica-aplicada-e-ingenier√≠a-de-sistemas"><span class="header-section-number">2.2.0.1</span> Ejemplos en Matem√°tica Aplicada e Ingenier√≠a de Sistemas</h4>
<ol type="1">
<li><strong>Modelado de tiempos de respuesta en redes</strong>
<ul>
<li><span class="math inline">\(X\)</span>: tiempo de llegada de paquetes (variable continua).<br>
</li>
<li><span class="math inline">\(P\)</span>: distribuci√≥n de retardo desconocida; objetivo: estimar par√°metros de una ley de colas M/M/1.</li>
</ul></li>
<li><strong>Estimaci√≥n de par√°metros en ecuaciones diferenciales estoc√°sticas</strong>
<ul>
<li><span class="math inline">\(X(t)\)</span>: trayectoria observada de un proceso de It√¥.<br>
</li>
<li><span class="math inline">\(P\)</span>: ley del proceso (por ejemplo, coeficientes de difusi√≥n y deriva), inferidos a partir de trayectorias discretas.</li>
</ul></li>
<li><strong>Calibraci√≥n de sensores en sistemas de control</strong>
<ul>
<li><span class="math inline">\(X\)</span>: lecturas del sensor (vector aleatorio).<br>
</li>
<li><span class="math inline">\(P\)</span>: distribuci√≥n conjunta desconocida de ruido; se estima para dise√±ar filtros de Kalman √≥ptimos.</li>
</ul></li>
</ol>
<hr>
<p>Con esta distinci√≥n clara entre dato observado y modelo probabil√≠stico, estamos listos para construir estimadores y desarrollar la inferencia estad√≠stica en las secciones siguientes.</p>
</section>
</section>
</section>
<section id="variable-aleatoria" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> VARIABLE ALEATORIA</h1>
<section id="variables-y-vectores-aleatorios" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="variables-y-vectores-aleatorios"><span class="header-section-number">3.1</span> Variables y vectores aleatorios</h2>
<p>Consideramos un experimento aleatorio cuyos resultados pertenecen al espacio muestral Œ©. Modelamos este proceso suponiendo que existe una terna <span class="math inline">\((\Omega, \mathcal{A}, P),\)</span> donde:</p>
<ul>
<li><span class="math inline">\(\Omega\)</span> es el espacio muestra,<br>
</li>
<li><span class="math inline">\(\mathcal{P}(\Omega)\)</span> es el conjunto de partes de Œ©,<br>
</li>
<li><span class="math inline">\(\mathcal{A}\in\mathcal{P}(\Omega)\)</span> es una œÉ-√°lgebra,<br>
</li>
<li><span class="math inline">\(P\colon \mathcal{A} \to [0,1]\)</span> es una medida de probabilidad que refleja las caracter√≠sticas aleatorias del experimento realizado.</li>
</ul>
<p>A esta terna se le llama <strong>espacio de probabilidad</strong>.</p>
<p>Los resultados de un experimento aleatorio no son analizados ‚Äúen bruto‚Äù, sino que se les da una representaci√≥n num√©rica que facilita su tratamiento. Esto se logra introduciendo variables aleatorias, que asocian cada resultado <span class="math inline">\(\omega\in \Omega\)</span> con un valor num√©rico o vectorial, y sobre las cuales luego aplicamos t√©cnicas de inferencia estad√≠stica.</p>
<p>En todo estudio estad√≠stico partimos de un <strong>experimento aleatorio</strong> cuyo conjunto de resultados posibles se denomina <strong>espacio muestral</strong> Œ©. Para cuantificar dichos resultados definimos las siguientes estructuras:</p>
<p><strong>Definici√≥n: Variables Aleatorias</strong></p>
<p>Sea <span class="math inline">\((\Omega,\mathcal{A},P)\)</span> un espacio de probabilidad. Una <strong>variable aleatoria</strong> es una funci√≥n <span class="math inline">\(X\colon (\Omega,\mathcal{A})\;\longrightarrow\; (\mathbb{R},\mathcal{B}),\)</span> tal que para todo <span class="math inline">\(B\in\mathcal{B}\)</span> (la <span class="math inline">\(\sigma\)</span>-√°lgebra de Borel en ‚Ñù), <span class="math inline">\(X^{-1}(B)\;=\;\{\omega\in\Omega : X(\omega)\in B\}\;\in\;\mathcal{A}.\)</span></p>
<ul>
<li><p>Si el espacio muestral Œ© es <strong>finito o numerable</strong>, diremos que es un espacio <strong>discreto</strong> y las variables aleatorias asociadas al experimento normalmente estar√°n definidas como <span class="math inline">\(X\colon \Omega \;\longrightarrow\; \mathbb{Z}.\)</span></p></li>
<li><p>Si <span class="math inline">\(\Omega\)</span> es <strong>no numerable</strong>, entonces diremos que es un espacio <strong>continuo</strong> y <span class="math inline">\(X\colon \Omega \;\longrightarrow\; \mathbb{R}.\)</span></p></li>
</ul>
<hr>
<p><strong>Definici√≥n: Vector aleatorio</strong></p>
<p>Un <strong>vector aleatorio</strong> de dimensi√≥n <span class="math inline">\(n\)</span> es <span class="math inline">\(\mathbf{X} = (X_1,\dots,X_n)\colon(\Omega,\mathcal{A})\longrightarrow(\mathbb{R}^n,\mathcal{B}^n),\)</span> donde cada componente <span class="math inline">\(X_i\)</span> es variable aleatoria y <span class="math inline">\(\mathcal{B}^n\)</span> la <span class="math inline">\(\sigma\)</span>-√°lgebra de Borel en ‚Ñù‚Åø.</p>
<hr>
<p><strong>Ejemplos</strong> <strong>Lanzamiento de dos monedas</strong></p>
<p>Sea <span class="math inline">\(\Omega =\{\,CC,\;C-,\;-C,\;--\},\)</span> donde <span class="math inline">\(C\)</span> = ‚Äúcara‚Äù y <span class="math inline">\(-\)</span> = ‚Äúcruz‚Äù. Podemos definir:<br>
<span class="math inline">\(X_1(\omega) = \text{n√∫mero de caras en }\omega.\)</span> <span class="math inline">\(X_2(\omega) = 2 - X_1(\omega)\;=\; \text{n√∫mero de cruces}.\)</span> <span class="math inline">\(X_3(\omega) = \bigl(X_1(\omega)\bigr)^2.\)</span></p>
<p>Entonces <span class="math inline">\((X_1,X_2,X_3)\)</span> es un vector aleatorio de dimensi√≥n 3.</p>
<p><strong>Tiempos de servicio en un servidor</strong></p>
<p>Sean <span class="math inline">\(T_i\)</span> los tiempos de servicio (en segundos) de las peticiones <span class="math inline">\(i=1,2,3\)</span>. Definimos<br>
<span class="math inline">\(\mathbf{T}=(T_1,T_2,T_3),\quad S = T_1 + T_2 + T_3,\quad M = \max\{T_1,T_2,T_3\}.\)</span></p>
<p><strong>Lecturas de sensores en red distribuida</strong></p>
<p>En tres nodos <span class="math inline">\(i=1,2,3\)</span> medimos temperatura <span class="math inline">\(X_{i,1}\)</span>, presi√≥n <span class="math inline">\(X_{i,2}\)</span> y humedad <span class="math inline">\(X_{i,3}\)</span>. El vector global es <span class="math inline">\(\mathbf{X} = (X_{1,1},X_{1,2},X_{1,3},\,X_{2,1},\dots,X_{3,3}) \in \mathbb{R}^9.\)</span></p>
<hr>
<p>Con estas definiciones rigurosas disponemos ya de los objetos b√°sicos para, en las siguientes secciones, construir estimadores, estudiar su comportamiento asint√≥tico y contrastar hip√≥tesis sobre la distribuci√≥n subyacente <span class="math inline">\(P\)</span>.</p>
</section>
<section id="distribuci√≥n-de-una-variable-aleatoria.-funciones-de-distribuci√≥n-de-probabilidad-y-de-densidad" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="distribuci√≥n-de-una-variable-aleatoria.-funciones-de-distribuci√≥n-de-probabilidad-y-de-densidad"><span class="header-section-number">3.2</span> Distribuci√≥n de una variable aleatoria. Funciones de distribuci√≥n, de probabilidad y de densidad</h2>
<p><strong>Distribuci√≥n de una Variable Aleatoria</strong></p>
<p>La realizaci√≥n de un experimento aleatorio da lugar a un resultado <span class="math inline">\(\omega\in\Omega\)</span> que es aleatorio. Por lo tanto, <span class="math inline">\(X(\omega)\)</span> es un valor de <span class="math inline">\(\mathbb{R}\)</span> tambi√©n aleatorio. Es decir, la variable aleatoria <span class="math inline">\(X\)</span> induce una medida de probabilidad en <span class="math inline">\(\mathbb{R}\)</span>. A esa medida de probabilidad se le llama <strong>distribuci√≥n de <span class="math inline">\(X\)</span></strong> o <strong>ley de <span class="math inline">\(X\)</span></strong>. Una de las formas de caracterizar la distribuci√≥n de una variable aleatoria es dar su funci√≥n de distribuci√≥n <span class="math inline">\(F_X\)</span>, que est√° definida as√≠:</p>
<p><span class="math inline">\(F_X(x) \;=\; P(X \le x)\;=\; P\bigl(\{\omega \in \Omega : X(\omega) \le x\}\bigr)\;=\; P\bigl(X^{-1}((-\infty, x])\bigr).\)</span>$</p>
<p>En el caso de que <span class="math inline">\(X\)</span> sea una <strong>variable aleatoria discreta</strong>, es decir, en el caso de que <span class="math inline">\(X\)</span> solo tome una cantidad finita o numerable de valores de <span class="math inline">\(\mathbb{R}\)</span>, su distribuci√≥n tambi√©n puede caracterizarse por su <strong>funci√≥n de probabilidad</strong> (o <strong>funci√≥n de masa de probabilidad</strong>) <span class="math inline">\(f_X\)</span>, definida como</p>
<p><span class="math display">\[f_X : \mathbb{R} \longrightarrow [0,1],\qquad f_X(x) = P(X = x).\]</span></p>
<p>Esa funci√≥n solo es no nula en un conjunto finito o numerable. Supondremos en adelante, sin p√©rdida de generalidad, que ese conjunto est√° contenido en <span class="math inline">\(\mathbb{Z}\)</span>. A partir de la funci√≥n de masa de probabilidad se puede calcular la probabilidad de que la variable aleatoria <span class="math inline">\(X\)</span> tome valores en cualquier elemento <span class="math inline">\(A \subseteq \mathbb{B}\)</span>:</p>
<p><span class="math inline">\(P(X \in A) = \sum_{x \in A} f_X(x).\)</span> me</p>
<p>La funci√≥n de distribuci√≥n y la funci√≥n de masa de probabilidad se relacionan de la siguiente forma:</p>
<p><span class="math inline">\(F_X(x) = \sum_{u \leq x} f_X(u), \quad f_X(x) = F_X(x) - F_X(x^-),\)</span> donde <span class="math inline">\(F_X(x^-) = \lim_{h \to 0^+} F_X(x - h)\)</span>.</p>
<p>Una clase relevante de variables aleatorias no discretas son las que poseen <strong>funci√≥n de densidad</strong>, es decir, aquellas cuya distribuci√≥n de probabilidad puede caracterizarse por una funci√≥n <span class="math inline">\(f_X(x) \geq 0\)</span> que cumple que:</p>
<p><span class="math inline">\(P(X \in A) = \int_{x \in A} f_X(x) \, dx, \quad \text{para todo } A \subseteq \mathbb{B}.\)</span></p>
<p>La relaci√≥n entre <span class="math inline">\(F_X\)</span> y <span class="math inline">\(f_X\)</span> es la siguiente:</p>
<p><span class="math inline">\(F_X(x) = \int_{-\infty}^{x} f_X(u) \, du, \quad f_X(x) = \frac{d}{dx} F_X(x),\)</span></p>
<p>salvo quiz√°s en un n√∫mero finito de puntos <span class="math inline">\(x \in \mathbb{R}\)</span>. Las variables aleatorias que poseen funci√≥n de densidad se llaman <strong>variables aleatorias absolutamente continuas</strong>. Abusando del lenguaje, aqu√≠ nos referiremos a ellas como variables aleatorias continuas.</p>
</section>
<section id="esperanza-y-varianza" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="esperanza-y-varianza"><span class="header-section-number">3.3</span> Esperanza y varianza</h2>
<p>Si se desea describir totalmente la distribuci√≥n de probabilidad de una variable aleatoria <span class="math inline">\(X\)</span> acabamos de ver que podemos dar su funci√≥n de distribuci√≥n o su funci√≥n de masa o de densidad, seg√∫n el caso. Una descripci√≥n parcial puede efectuarse calculando algunas caracter√≠sticas de la variable aleatoria <span class="math inline">\(X\)</span>, como por ejemplo medidas de posici√≥n o de dispersi√≥n. Estudiaremos algunas de ellas.</p>
<p>Se define la <strong>esperanza</strong> de una variable aleatoria <span class="math inline">\(X\)</span> como la integral de Lebesgue de <span class="math inline">\(X\)</span>:</p>
<p><span class="math inline">\(E(X) = \int_{\Omega} X(w) dP(w).\)</span></p>
<p>En el caso de variables aleatorias discretas la esperanza puede calcularse como:</p>
<p><span class="math inline">\(E(X) = \sum_{w \in \Omega} X(w) P(w) = \sum_{k \in \mathbb{Z}} k P(X = k) = \sum_{k \in \mathbb{Z}} k f_X(k).\)</span></p>
<p>Por otro lado, la esperanza de una variable aleatoria continua se puede calcular as√≠:</p>
<p><span class="math inline">\(E(X) = \int_{\mathbb{R}} x f_X(x) dx.\)</span></p>
<p>La esperanza de una variable aleatoria <span class="math inline">\(X\)</span> es una medida de posici√≥n de <span class="math inline">\(X\)</span>: es el centro de gravedad de la distribuci√≥n de probabilidad de <span class="math inline">\(X\)</span>.</p>
<p>Si <span class="math inline">\(h\)</span> es una funci√≥n medible <span class="math inline">\(h : \mathbb{R} \rightarrow \mathbb{R}\)</span>, entonces <span class="math inline">\(Y = h(X)\)</span> es tambi√©n variable aleatoria y su esperanza se puede calcular a partir de la distribuci√≥n de <span class="math inline">\(X\)</span>:</p>
<p><span class="math inline">\(E(h(X)) = \int_{\Omega} h(X(w)) dP(w)\)</span> que en el caso de que <span class="math inline">\(X\)</span> sea discreta puede reescribirse como</p>
<p><span class="math inline">\(E(h(X)) = \sum_{k \in \mathbb{Z}} h(k) f_X(k).\)</span></p>
<p>Si <span class="math inline">\(X\)</span> es una variable aleatoria continua entonces</p>
<p><span class="math inline">\(E(h(X)) = \int_{\mathbb{R}} h(x) f_X(x) dx.\)</span></p>
<p>Si existe <span class="math inline">\(\mu = E(X)\)</span> y es finita puede definirse una medida de dispersi√≥n de la variable aleatoria <span class="math inline">\(X\)</span> a partir de una transformaci√≥n <span class="math inline">\(h\)</span> de <span class="math inline">\(X\)</span>. Es lo que se denomina <strong>varianza</strong> de <span class="math inline">\(X\)</span> y se define as√≠:</p>
<p><span class="math inline">\(V(X) = E((X - \mu)^2) = E(X^2) - \mu^2 = E(X^2) - (E(X))^2.\)</span></p>
</section>
<section id="funci√≥n-generadora-de-momentos" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="funci√≥n-generadora-de-momentos"><span class="header-section-number">3.4</span> Funci√≥n generadora de momentos</h2>
<p>Dada una variable aleatoria <span class="math inline">\(X\)</span>, o su funci√≥n de distribuci√≥n <span class="math inline">\(F\)</span>, vamos a definir otra funci√≥n generadora, como</p>
<p><span class="math inline">\(M_X(t) = \mathbb{E}(e^{tX}),\)</span> siempre que este valor esperado exista.</p>
<p>Notemos que cuando <span class="math inline">\(X\)</span> toma valores en los enteros no-negativos, <span class="math inline">\(M_X(t) = \phi_X(e^t)\)</span>, donde <span class="math inline">\(\phi_X(s)=E[s^X]=\sum_{k=0}^{\infty}p_ks^k\)</span> para <span class="math inline">\(s\in[0,1]\)</span> es la funci√≥n generadora de probabilidad (f.g.p.) de la variable <span class="math inline">\(X\)</span>, con <span class="math inline">\(p_k=P(X=k)\)</span>. Si <span class="math inline">\(X\)</span> est√° acotada, <span class="math inline">\(M_X\)</span> est√° bien definida para todo <span class="math inline">\(t\)</span> real; en cambio, si <span class="math inline">\(X\)</span> no est√° acotada, es posible que el dominio de <span class="math inline">\(M_X\)</span> no sea el conjunto de todos los reales. En todo caso, <span class="math inline">\(\phi\)</span> siempre est√° definida en cero, y <span class="math inline">\(M(0) = 1\)</span>.</p>
<p>Es posible demostrar que si la f.g.m. de la v.a. <span class="math inline">\(X\)</span> existe en un entorno de 0, entonces para todo <span class="math inline">\(k &gt; 0\)</span>,</p>
<p><span class="math inline">\(\mathbb{E}[|X|^k] &lt; \infty.\)</span></p>
<p>M√°s a√∫n, la serie</p>
<p><span class="math inline">\(M_X(t) =
\mathbb{E}(e^{tX})
= \mathbb{E}\left(1 + \sum_{k=1}^{\infty} \frac{t^k X^k}{k!}\right)
= 1 + \sum_{n=1}^{\infty} \frac{t^k}{k!} \mathbb{E}(X^k)
\tag{5.1}\)</span></p>
<p>es convergente y se puede derivar t√©rmino a t√©rmino. Obtenemos</p>
<p><span class="math inline">\(M'_X(0) = \mathbb{E}(X); \quad M''_X(0) = \mathbb{E}(X^2)\)</span></p>
<p>y en general</p>
<p><span class="math inline">\(M_X^{(k)}(0) = \mathbb{E}(X^k).\)</span></p>
<p>Es por esta √∫ltima propiedad que esta funci√≥n se conoce como <strong>funci√≥n generadora de momentos</strong> (f.g.m.).</p>
<p>üé≤ Ejemplo: f.g.m. de la distribuci√≥n Binomial</p>
<p>Sea <span class="math inline">\(X \sim \text{Binomial}(n, p)\)</span>, es decir, la suma de <span class="math inline">\(n\)</span> ensayos de Bernoulli con probabilidad de √©xito <span class="math inline">\(p\)</span>. La funci√≥n generadora de momentos es: <a href="https://github.com/BMSS-EAFIT-Courses/inference-statistic/blob/main/mgf_binomial.r">Ejemplo fgm binomial</a></p>
<p><span class="math inline">\(M_X(t) = \mathbb{E}[e^{tX}] = (1 - p + p e^t)^n\)</span></p>
<pre> ```r #
mgf_binomial &lt;- function(t, n = 10, p = 0.3) {
  (1 - p + p * exp(t))^n
}

t_vals &lt;- seq(-3, 3, length.out = 300)
mgf_vals &lt;- sapply(t_vals, mgf_binomial)

plot(t_vals, mgf_vals, type = "l", lwd = 2,
     main = expression("F.G.M. para X ~ Binomial(10, 0.3)"),
     xlab = "t", ylab = expression(M[X](t)))
grid()
``` </pre>
<p>üìà Ejemplo: f.g.m. de la distribuci√≥n Normal Est√°ndar</p>
<p>Sea <span class="math inline">\(X \sim \mathcal{N}(0, 1)\)</span>. Su funci√≥n generadora de momentos es:</p>
<p><span class="math inline">\(M_X(t) = \mathbb{E}[e^{tX}] = e^{\frac{t^2}{2}}\)</span></p>
Esta expresi√≥n se obtiene usando la forma cerrada del momento de una normal est√°ndar.
<pre> ```r #
mgf_normal &lt;- function(t) {
  exp(t^2 / 2)
}

t_vals &lt;- seq(-3, 3, length.out = 300)
mgf_vals &lt;- sapply(t_vals, mgf_normal)

plot(t_vals, mgf_vals, type = "l", lwd = 2,
     main = expression("F.G.M. para X ~ N(0, 1)"),
     xlab = "t", ylab = expression(M[X](t)))
grid()
``` </pre>
<p><strong>‚ùì Preguntas gu√≠a sobre la gr√°fica de la funci√≥n generadora de momentos</strong></p>
<p><strong>üìå ¬øQu√© representa la gr√°fica de la f.g.m. <span class="math inline">\(M_X(t)\)</span>?</strong></p>
<p>La gr√°fica muestra c√≥mo evoluciona el valor esperado de <span class="math inline">\(e^{tX}\)</span> cuando <span class="math inline">\(t\)</span> var√≠a. Esta funci√≥n codifica <strong>todos los momentos de la variable aleatoria</strong> <span class="math inline">\(X\)</span>, y por tanto, contiene informaci√≥n completa sobre su distribuci√≥n (si existe un entorno donde la f.g.m. es finita).</p>
<hr>
<p><strong>üß≠ ¬øQu√© se observa en la f.g.m. de una distribuci√≥n Binomial?</strong> <img width="480" height="480" alt="mgf_binomial" src="https://github.com/user-attachments/assets/8524edee-0b5c-4938-a9e8-8af0e8c5840c"></p>
<p>![Gr√°fica MGF Binomial]</p>
<p>#Preguntas y respuestas</p>
<ul>
<li><p><strong>¬øC√≥mo es el comportamiento de la f.g.m. cerca de <span class="math inline">\(t = 0\)</span>?</strong></p>
<p>En <span class="math inline">\(t = 0\)</span>, siempre se cumple que <span class="math inline">\(M_X(0) = 1\)</span>, ya que:</p>
<p><span class="math inline">\(M_X(0) = \mathbb{E}[e^{0 \cdot X}] = \mathbb{E}[1] = 1\)</span></p></li>
<li><p><strong>¬øQu√© indica la curvatura de la gr√°fica?</strong></p>
<p>La curvatura refleja el crecimiento exponencial de los momentos. Si la curva crece r√°pidamente hacia la derecha, significa que los momentos (media, varianza, etc.) tambi√©n crecen con rapidez.</p></li>
<li><p><strong>¬øPor qu√© la gr√°fica es convexa?</strong></p>
<p>Todas las funciones generadoras de momentos son <strong>estrictamente convexas</strong> en el intervalo donde est√°n definidas. Esto es una consecuencia de que derivadas sucesivas representan momentos positivos.</p></li>
<li><p><strong>¬øQu√© pasa si cambio los par√°metros <span class="math inline">\(n\)</span> y <span class="math inline">\(p\)</span>?</strong></p>
<p>Aumentar $ n $ o <span class="math inline">\(p\)</span> tiende a <strong>elevar</strong> la f.g.m. en el lado derecho, reflejando una mayor media y varianza.</p></li>
</ul>
<hr>
<p><strong>üìà ¬øC√≥mo se comporta la f.g.m. para la Normal Est√°ndar?</strong> <img width="480" height="480" alt="mgf_normal" src="https://github.com/user-attachments/assets/e877129d-4235-42d9-baff-7f29d3afc4a7"></p>
<p>![Gr√°fica MGF Normal]</p>
<p><strong>Preguntas y respuestas</strong></p>
<ul>
<li><p><strong>¬øPor qu√© es sim√©trica respecto al eje $ t = 0 $?</strong></p>
<p>Porque la normal est√°ndar es sim√©trica alrededor de su media $ = 0 $, y su f.g.m. tiene la forma:</p></li>
</ul>
<p><span class="math inline">\(M_X(t) = e^{t^2 / 2}\)</span></p>
<p>lo cual es una funci√≥n par: <span class="math inline">\(M_X(-t)= M_X(t)\)</span>.</p>
<ul>
<li><p><strong>¬øQu√© tan r√°pido crece la funci√≥n?</strong></p>
<p>Muy r√°pido. El crecimiento es exponencial cuadr√°tico. Esto implica que los momentos de la normal crecen r√°pidamente en magnitud.</p></li>
<li><p><strong>¬øC√≥mo se relaciona esta gr√°fica con los momentos de la normal?</strong></p>
<p>Derivando sucesivamente la f.g.m. en $ t = 0 $, se obtiene:</p>
<p><span class="math inline">\(\mathbb{E}[X^k] = M_X^{(k)}(0)\)</span></p>
<p>Por tanto, la gr√°fica ‚Äúencierra‚Äù toda la informaci√≥n sobre los momentos.</p></li>
</ul>
<hr>
<p><strong>üß† Conclusi√≥n</strong></p>
<p>Estas gr√°ficas te permiten <strong>visualizar la informaci√≥n estad√≠stica codificada en una variable aleatoria</strong>. La f.g.m. no es solo una herramienta algebraica para obtener momentos, sino una forma poderosa de describir el comportamiento global de la variable.</p>
<blockquote class="blockquote">
<p><strong>¬øQu√© pasa si dos variables tienen la misma f.g.m.?</strong><br>
¬°Tienen la misma distribuci√≥n! (si la f.g.m. est√° definida en un entorno de 0).</p>
</blockquote>
<p><strong>Ejemplo: Distribuci√≥n uniforme <span class="math inline">\(U(a,b)\)</span></strong></p>
<p>Si <span class="math display">\[X \sim U(a,b),\]</span><br>
su densidad es<br>
<span class="math display">\[f(x) = \frac{1}{b - a}\quad\text{para }a &lt; x &lt; b,\]</span><br>
y su funci√≥n generadora de momentos viene dada por</p>
<a id="eq:5.2"></a>
<table align="center">
<tbody><tr>
<td align="center">
<span class="math display">\[M(t)= \int_a^b \frac{e^{t x}}{b - a}\,dx= \frac{e^{b t} - e^{a t}}{t\,(b - a)}.\]</span><br>

</td>
<td valign="bottom">
(5.2)
</td>
</tr>
</tbody></table>
<p>En el caso particular de la distribuci√≥n uniforme en <span class="math inline">\((0,1)\)</span> se obtiene<br>
<span class="math display">\[M(t) = \frac{e^t - 1}{t}.\]</span></p>
<hr>
<p>Para derivar la f√≥rmula <a href="#eq:5.2">#(5.2)</a> y obtener los momentos, podemos usar el desarrollo en serie de la funci√≥n exponencial:</p>
<p><span class="math display">\[M(t)= \frac{1}{t\,(b - a)}\bigl(e^{b t} - e^{a t}\bigr) \\
= \frac{1}{t\,(b - a)}\Bigl[\bigl(1 + \sum_{n=1}^\infty \tfrac{(b t)^n}{n!}\bigr)
                    -\bigl(1 + \sum_{n=1}^\infty \tfrac{(a t)^n}{n!}\bigr)\Bigr] \\
= \frac{1}{b - a}\sum_{n=1}^\infty \frac{b^n - a^n}{n!}\,t^{n-1}.
\]</span></p>
<p>Este es el desarrollo de Maclaurin de <span class="math inline">\(M(t)\)</span> en <span class="math inline">\(t=0\)</span>; por tanto, sus derivadas en cero satisfacen</p>
<table align="center">
<tbody><tr>
<td align="center">
<span class="math display">\[M^{(k)}(0)= \frac{b^{k+1} - a^{k+1}}{(k+1)\,(b - a)}.\]</span>
</td>
<td valign="bottom">
(5.3)
</td>
</tr>
</tbody></table>
<p>En particular:</p>
<ul>
<li><p><span class="math display">\[M'(0)= \frac{b^2 - a^2}{2\,(b - a)}= \frac{a + b}{2},\]</span> que coincide con <span class="math inline">\(\mathbb{E}(X)\)</span>.</p></li>
<li><p><span class="math display">\[M''(0)= \frac{b^3 - a^3}{3\,(b - a)}= \frac{a^2 + a b + b^2}{3},\]</span></p></li>
</ul>
<p>y un c√°lculo directo muestra que la varianza es</p>
<p><span class="math display">\[\mathrm{Var}(X)= \mathbb{E}(X^2) - \bigl(\mathbb{E}(X)\bigr)^2= \frac{(a + b)^2}{12}.\]</span></p>
<p><strong>Observaci√≥n importante</strong> Sea <span class="math inline">\(X\)</span> una v.a. con f.g.m. <span class="math inline">\(M_X\)</span> y sea <span class="math inline">\(Y=aX+b\)</span> una transformaci√≥n lineal de <span class="math inline">\(X\)</span>, entonces</p>
<p><span class="math display">\[M_Y(t)=E(e^{tY})=E(e^{t(aX+b)})=E(e^{taX}e^{tb})=e^{tb}E(e^{taX})=e^{tb}M_X(at)\]</span></p>
<p><strong>Teorema (fgm de suma de v.a.s)</strong></p>
<p>Si <span class="math inline">\(X\)</span> tiene funci√≥n generadora de momentos <span class="math inline">\(M(t)\)</span> que est√° definida en un entorno <span class="math inline">\((-a,a)\)</span> de 0, entonces <span class="math inline">\(M(t)\)</span> caracteriza a la distribuci√≥n de <span class="math inline">\(X\)</span>; es decir, si otra variable <span class="math inline">\(Y\)</span> tiene la misma funci√≥n generadora de momentos, las distribuciones de <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> coinciden.</p>
<hr>
Si <span class="math inline">\(X,Y\)</span> son variables aleatorias con funciones generadoras de momentos respectivas <span class="math inline">\(M_X\)</span> y <span class="math inline">\(M_Y\)</span> que existen en un dominio com√∫n <span class="math inline">\(|t| &lt; d\)</span>, entonces la f.g.m. de la suma <span class="math inline">\(X+Y\)</span> est√° dada por <a id="eq:5.5"></a>
<table align="center">
<tbody><tr>
<td align="center">
<span class="math display">\[M_{X+Y}(t)= \mathbb{E}\bigl[e^{t(X+Y)}\bigr]= \mathbb{E}\bigl[e^{tX}\,e^{tY}\bigr]=\mathbb{E}\bigl[e^{tX}\bigr]\;\mathbb{E}\bigl[e^{tY}\bigr]= M_X(t)\,M_Y(t).
\tag{5.5}
\]</span>
</td>
<td valign="bottom">
(5.5)
</td>
</tr>
</tbody></table>
<p>Este resultado se extiende a la suma de <span class="math inline">\(n\)</span> variables aleatorias independientes. Si</p>
<p><span class="math display">\[S_n = X_1 + \cdots + X_n,\]</span></p>
<p>entonces</p>
<p><span class="math display">\[M_{S_n}(t)= \mathbb{E}\bigl[e^{tS_n}\bigr]= \mathbb{E}\Bigl[e^{t\sum_{i=1}^n X_i}\Bigr]= \prod_{i=1}^n\mathbb{E}\bigl[e^{tX_i}\bigr]= \prod_{i=1}^n M_{X_i}(t).\]</span></p>
<p>La funci√≥n generadora de momentos resulta particularmente √∫til cuando consideramos sucesiones de variables aleatorias, como lo muestra el siguiente teorema que enunciamos sin demostraci√≥n:</p>
<hr>
<p><strong>Teorema (de Continuidad)</strong></p>
<p>Sea <span class="math inline">\(F_n(x)\)</span>, <span class="math inline">\(n\ge1\)</span>, una sucesi√≥n de funciones de distribuci√≥n con funciones generadoras de momentos respectivas <span class="math inline">\(M_n(t)\)</span>, definidas en <span class="math inline">\(|t|&lt;b\)</span>. Supongamos que cuando <span class="math inline">\(n\to\infty\)</span>,</p>
<p><span class="math display">\[
M_n(t)\,\longrightarrow\,M(t)
\quad\text{para }|t|\le a,
\]</span></p>
<p>donde <span class="math inline">\(M(t)\)</span> es la funci√≥n generadora de momentos de la distribuci√≥n l√≠mite <span class="math inline">\(F(x)\)</span>. Entonces</p>
<p><span class="math display">\[
F_n(x)\,\longrightarrow\,F(x)
\quad\text{cuando }n\to\infty
\]</span></p>
<p>para todo punto <span class="math inline">\(x\)</span> en el cual <span class="math inline">\(F\)</span> es continua.</p>
<p><strong>Teorema de Laplace‚ÄìMoivre</strong></p>
<p>Sea <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> una sucesi√≥n de variables aleatorias <strong>i.i.d.</strong> con distribuci√≥n ( (p) ), donde ( 0 &lt; p &lt; 1 ). Sea:</p>
<p><span class="math display">\[
S_n = X_1 + X_2 + \cdots + X_n \sim \text{Binomial}(n, p)
\]</span></p>
<p>y consideremos la variable tipificada:</p>
<p><span class="math display">\[
Z_n = \frac{S_n - np}{\sqrt{np(1 - p)}}
\]</span></p>
<p>Entonces, cuando ( n ), se tiene convergencia en distribuci√≥n a una normal est√°ndar:</p>
<p><span class="math display">\[
Z_n \xrightarrow{d} \mathcal{N}(0, 1)
\]</span></p>
<p>es decir,</p>
<p><span class="math display">\[
\lim_{n \to \infty} \mathbb{P}(Z_n \leq z) = \Phi(z), \quad \text{para todo } z \in \mathbb{R}
\]</span></p>
<p>donde ( (z) ) es la funci√≥n de distribuci√≥n acumulada de la normal est√°ndar.</p>
<hr>
<p><strong>Demostraci√≥n usando funciones generadoras de momentos</strong></p>
<p>La funci√≥n generadora de momentos (mgf) de <span class="math inline">\(S_n \sim \text{Binomial}(n, p)\)</span> es:</p>
<p><span class="math display">\[
M_{S_n}(t) = \left(1 - p + p e^t\right)^n
\]</span></p>
<p>Queremos obtener la mgf de la variable tipificada ( Z_n ). Usamos la propiedad de cambio de variable de la mgf:</p>
<p><span class="math display">\[
M_{Z_n}(t) = \mathbb{E}\left[ e^{t Z_n} \right]
= \mathbb{E}\left[ e^{t \cdot \frac{S_n - np}{\sqrt{np(1 - p)}}} \right]
= e^{-t \cdot \frac{np}{\sqrt{np(1 - p)}}} \cdot M_{S_n}\left( \frac{t}{\sqrt{np(1 - p)}} \right)
\]</span></p>
<p>Sustituimos la mgf de ( S_n ):</p>
<p><span class="math display">\[
M_{Z_n}(t) = \exp\left( -t \cdot \frac{np}{\sqrt{np(1 - p)}} \right)
\cdot \left( 1 - p + p e^{t / \sqrt{np(1 - p)}} \right)^n
\]</span></p>
<hr>
<p><strong>Aproximaci√≥n por series de Taylor</strong></p>
<p>Expandimos <span class="math inline">\(e^{t / \sqrt{np(1 - p)}}\)</span> para <span class="math inline">\(n\)</span> grande:</p>
<p><span class="math display">\[
e^{t / \sqrt{np(1 - p)}} = 1 + \frac{t}{\sqrt{np(1 - p)}} + \frac{t^2}{2np(1 - p)} + \cdots
\]</span></p>
<p>Entonces:</p>
<p><span class="math display">\[
1 - p + p e^{t / \sqrt{np(1 - p)}} \approx 1 + \frac{pt}{\sqrt{np(1 - p)}} + \frac{pt^2}{2np(1 - p)} + \cdots
\]</span></p>
<p>Usamos que <span class="math inline">\(\log(1 + x) \approx x - \frac{x^2}{2} + \cdots\)</span> para <span class="math inline">\(x \approx 0\)</span>:</p>
<p><span class="math display">\[\log M_{Z_n}(t) \approx -t \cdot \frac{np}{\sqrt{np(1 - p)}}+ n \left( \frac{pt}{\sqrt{np(1 - p)}} + \frac{pt^2}{2np(1 - p)} \right)\]</span></p>
<p>Simplificamos:</p>
<ul>
<li>El t√©rmino lineal se cancela:</li>
</ul>
<p><span class="math display">\[
-t \cdot \frac{np}{\sqrt{np(1 - p)}} + n \cdot \frac{pt}{\sqrt{np(1 - p)}} = 0
\]</span></p>
<ul>
<li>Queda:</li>
</ul>
<p><span class="math display">\[
\log M_{Z_n}(t) \to \frac{t^2}{2}, \quad \text{cuando } n \to \infty
\]</span></p>
<p>Por tanto:</p>
<p><span class="math display">\[
M_{Z_n}(t) \to e^{t^2 / 2}
\]</span></p>
<hr>
<p><strong>Conclusi√≥n</strong></p>
<p>Como <span class="math inline">\(e^{t^2/2}\)</span> es la mgf de <span class="math inline">\(\mathcal{N}(0, 1)\)</span>, y por el teorema de unicidad de la funci√≥n generadora de momentos:</p>
<p><span class="math display">\[
Z_n \xrightarrow{d} \mathcal{N}(0, 1)
\]</span></p>
<p>Esto concluye la demostraci√≥n del <strong>Teorema de Laplace‚ÄìMoivre</strong> utilizando funciones generadoras de momentos.</p>
</section>
<section id="muestra-aleatoria-simple" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="muestra-aleatoria-simple"><span class="header-section-number">3.5</span> Muestra aleatoria simple</h2>
<p>Sea <span class="math inline">\(\underset{\sim}{X} =(X_1 ,..., X_n)\)</span> un vector aleatorio. Se dice que sus componentes <span class="math inline">\(X_1 ,..., X_n\)</span> son si <span class="math inline">\(P(X_1\leq x_1 ,..., X_n\leq x_n)=P(X_1\leq x_1)...P(X_n\leq x_n)\)</span> para cualesquiera valores <span class="math inline">\(x_1,..., x_n\)</span> .</p>
<p>Si adem√°s la distribuci√≥n de las <span class="math inline">\(n\)</span> variables aleatorias <span class="math inline">\(X_i\)</span> es la misma, se dice que <span class="math inline">\(X_1 ,...,X_n\)</span> son variables aleatorias <strong>independientes e id√©nticamente distribuidas</strong>, o bien que son v.a.i.i.d o simplemente i.i.d.</p>
<p>Si <span class="math inline">\(\underset{\sim}{X} =(X_1 ,..., X_n)\)</span> y <span class="math inline">\(X_1 ,..., X_n\)</span> son i.i.d. con funci√≥n de densidad (en su caso, de masa) <span class="math inline">\(f_X\)</span> , la distribuci√≥n conjunta de <span class="math inline">\(\underset{\sim}{X}\)</span> viene dada por la funci√≥n de densidad (en su caso, de masa) conjunta <span class="math display">\[
\begin{align*}
f_{\underset{\sim}{X}}(\underset{\sim}{x})&amp;=f_{(X_1 ,..., X_n)}(x_1 ,..., x_n)\\
&amp;=f_{(X_1)}(x_1)...f_{(X_n)}(x_n)\\
&amp;=\prod_{i=1}^{n}f_{(X_i)}(x_i)
\end{align*}
\]</span></p>
<p>A un vector <span class="math inline">\(\underset{\sim}{X} =(X_1 ,..., X_n)\)</span> de v.a.i.i.d. con distribuci√≥n igual a la de la variable aleatoria <span class="math inline">\(X\)</span> se le denomina tambi√©n <strong>muestra aleatoria simple</strong> de <span class="math inline">\(X\)</span> (m.a.s de <span class="math inline">\(X\)</span>).</p>
<p>Esto responde al hecho siguiente. Supongamos que se desea estudiar la caracter√≠stica <span class="math inline">\(X\)</span> de los individuos de una poblaci√≥n de tama√±o infinito. Definimos el experimento consistente en elegir aleatoriamente un individuo de la poblaci√≥n y llamamos <span class="math inline">\(X\)</span> al valor de la caracter√≠stica de inter√©s en ese individuo. X es una variable aleatoria.</p>
<p>Si definimos un nuevo experimento consistente en elegir una muestra aleatoria de n individuos y se anota <span class="math inline">\(X_i\)</span>, el valor de la caracter√≠stica en el individuo i-√©simo, entonces <strong>X</strong> <span class="math inline">\(=(X_1 ,..., X_n)\)</span> es una colecci√≥n de n v.a.i.i.d. con distribuci√≥n igual a la de la variable aleatoria <span class="math inline">\(X\)</span>, es decir, <span class="math inline">\(X_1 ,..., X_n\)</span> es una m.a.s. de X.</p>
</section>
<section id="modelo-param√©trico" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="modelo-param√©trico"><span class="header-section-number">3.6</span> Modelo param√©trico</h2>
<p>Usualmente la ley de probabilidad de una variable aleatoria se supone perteneciente a un modelo matem√°tico que depende s√≥lo de un n√∫mero finito de par√°metros: <span class="math inline">\(f_X \in\{f(x|\theta):\theta \in \Theta \subseteq \mathbb{R}^k\}\)</span>. Escribiremos alternativamente <span class="math inline">\(f(x;\theta)\)</span>, <span class="math inline">\(f(x|\theta)\)</span> o <span class="math inline">\(f_\theta(x)\)</span>.</p>
<p><strong>Definici√≥n</strong> El conjunto de distribuciones dadas por <span class="math inline">\(f_\theta(x)\)</span>, <span class="math inline">\(\theta \in \Theta\)</span> se llama familia param√©trica de distribuciones. <span class="math inline">\(\Theta\)</span> es el conjunto de par√°metros.</p>
<p><strong>Definici√≥n</strong> La correspondiente distribuci√≥n conjunta de una muestra aleatoria simple de <span class="math inline">\(X\)</span> viene dada por la funci√≥n de densidad (o funci√≥n de masa de probabilidad, seg√∫n el caso)</p>
<p><span class="math display">\[
f_{\underset{\sim}{X}}(\underset{\sim}{x} \mid \theta) = \prod_{i=1}^{n} f_{\theta}(x_i)
\]</span></p>
<p>A esta funci√≥n la llamaremos <strong>funci√≥n de verosimilitud</strong> de la muestra <span class="math inline">\(X_{\sim}\)</span>. Utilizaremos este t√©rmino para referirnos indistintamente a la funci√≥n de densidad conjunta (si las variables aleatorias son continuas) o a la funci√≥n de masa conjunta (si son discretas).</p>
</section>
<section id="sumas-de-variables-aleatorias" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="sumas-de-variables-aleatorias"><span class="header-section-number">3.7</span> Sumas de variables aleatorias</h2>
<p>Cuando se obtiene una muestra aleatoria simple <span class="math inline">\(X_{1},X_{2},\ldots,X_{n}\)</span> normalmente se calculan a partir de ellas cantidades que resumen los valores observados. Cualquiera de estos res√∫menes se puede expresar como una funci√≥n <span class="math inline">\(T(x_1,\ldots,x_n)\)</span> definida en el espacio <span class="math inline">\(\mathcal{X}^n\subseteq\mathbb{R}^n\)</span> donde est√°n las im√°genes del vector <span class="math inline">\((X_{1},X_{2},\ldots,X_{n})\)</span>.</p>
<p>Esta funci√≥n <span class="math inline">\(T\)</span> puede devolver valores de <span class="math inline">\(\mathbb{R}\)</span>, <span class="math inline">\(\mathbb{R}^2\)</span> o, en general, <span class="math inline">\(\mathbb{R}^k\)</span>.</p>
<p><span class="math display">\[T(X_1 , \ldots, X_n)=\sum_{i=1}^{n}X_i,\bar{X},\bar{X}+3, \min{X_1 , \ldots, X_n},\]</span> <span class="math display">\[T(X_1 , \ldots, X_n)=\left(\sum_{i=1}^{n}X_i,\sum_{i=1}^{n}(X_i-\bar{X})^2\right),\]</span> <span class="math display">\[T(X_1 , \ldots, X_n)=\left(\min\{X_1 , \ldots, X_n\},\sum_{i=1}^{n}X_i,\sum_{i=1}^{n}(X_i-\bar{X})^2\right),\]</span> <span class="math display">\[T(X_1 , \ldots, X_n)= (X_1 , \ldots, X_n)\]</span></p>
<p><strong>Definici√≥n de estad√≠sticos:</strong> Las funciones <span class="math inline">\(T\)</span> que dependen de una muestra aleatoria simple <span class="math inline">\(X_1 , \ldots, X_n\)</span> se llaman <strong>estad√≠sticos</strong>. Dependen de los valores observados, pero no de los par√°metros desconocidos que determinan la distribuci√≥n de <span class="math inline">\(X_i\)</span> .</p>
<p>Cuando un estad√≠stico <span class="math inline">\(T\)</span> es utilizado con el prop√≥sito de estimar un par√°metro <span class="math inline">\(\theta\)</span> diremos que <span class="math inline">\(T\)</span> es un estimador de <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Ejemplo de estad√≠stico</strong></p>
<p><span class="math inline">\(T(X_1 , \ldots, X_n)=\bar{X}\)</span> es un estimador de <span class="math inline">\(E(X)=\mu\)</span>.</p>
<p>En inferencia estad√≠stica interesa saber qu√© estad√≠sticos son suficientes para recoger toda la informaci√≥n que la muestra aporta sobre la distribuci√≥n de la variable aleatoria X muestreada. La respuesta depende de la distribuci√≥n de X.</p>
<p><strong>Definici√≥n distribuci√≥n en el muestreo:</strong> Dado que <span class="math inline">\(\underset{\sim}{X} =(X_1 ,..., X_n)\)</span> es una variable aleatoria, se tiene que <span class="math inline">\(Y=T(\underset{\sim}{X})=T(X_1 ,..., X_n)\)</span> ser√° tambi√©n una variable aleatoria. La ley de probabilidad de <span class="math inline">\(Y\)</span> se denomina <strong>distribuci√≥n en el muestreo de <span class="math inline">\(Y\)</span></strong> (o distribuci√≥n muestral). Los siguientes resultados dan informaci√≥n sobre algunas caracter√≠sticas de estad√≠sticos definidos a partir de sumas de variables aleatorias.</p>
</section>
<section id="estad√≠sticos-definidos-a-partir-de-sumas-de-variables-aleatorias" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="estad√≠sticos-definidos-a-partir-de-sumas-de-variables-aleatorias"><span class="header-section-number">3.8</span> Estad√≠sticos definidos a partir de sumas de variables aleatorias</h2>
<p><strong>Teorema</strong> Sean <span class="math inline">\(X_1,\ldots, X_n\)</span>,n n√∫meros reales, sea <span class="math inline">\(\bar{x} = \frac{1}{n} \sum_{i=1}^{n}x_i\)</span> su media aritm√©tica y sea <span class="math inline">\(S_n^2=\frac{\sum_{i=1}^{n}(x_i-\bar{x})^2}{n-1}\)</span> su varianza muestral.</p>
<ul>
<li><span class="math inline">\(\min_a\sum_{i=1}^{n}(x_i-a)^2=\sum_{i=1}^{n}(x_i-\bar{x})^2\)</span></li>
<li><span class="math inline">\((n-1)S_n^2=\sum_{i=1}^{n}(x_i-\bar{x})^2=\sum_{i=1}^{n}x_i^2-n\bar{x}^2\)</span></li>
</ul>
<p><strong>Lema</strong> Sea <span class="math inline">\(X_1,\ldots, X_n\)</span> una muestra aleatoria simple de <span class="math inline">\(X\)</span> y sea <span class="math inline">\(g(x)\)</span> una funci√≥n tal que <span class="math inline">\(E(g(X))\)</span> y <span class="math inline">\(Var(g(X))\)</span> existen. Entonces,</p>
<ul>
<li><span class="math inline">\(E(\sum_{i=1}^{n}g(X_i))=nE(g(X))\)</span>,</li>
<li><span class="math inline">\(Var(\sum_{i=1}^{n}g(X_i))=nVar(g(X))\)</span>.</li>
</ul>
<p>Para la demostraci√≥n ver G√≥mez et al.&nbsp;(2006)</p>
<p><strong>Teorema</strong> Sea <span class="math inline">\(X 1,\ldots, X_n\)</span> una muestra aleatoria simple de una poblaci√≥n <span class="math inline">\(X\)</span> con esperanza <span class="math inline">\(\mu\)</span> y varianza <span class="math inline">\(\sigma^2 &lt; \infty\)</span>. Sean <span class="math display">\[
        \begin{align*}
        &amp;\bar{X}=\frac{1}{n}\sum_{i=1}^{n}X_i,\ \
        S^2=\frac{\sum_{i=1}^{n}(X_i-\bar{X})^2}{n-1},
        \end{align*}    
\]</span> la media y la varianza muestrales, respectivamente. Entonces,</p>
<ol type="a">
<li><span class="math inline">\(E(\bar{X}) = \mu,\)</span></li>
<li><span class="math inline">\(Var(\bar{X}) = \frac{\sigma^2}{n},\)</span></li>
<li><span class="math inline">\(E(S^2) = \sigma^2\)</span>.</li>
</ol>
<p><strong>Teorema</strong> Sea <span class="math inline">\(X 1,\ldots, X_n\)</span> una muestra aleatoria simple de una poblaci√≥n <span class="math inline">\(X\)</span> con funci√≥n generadora de momentos <span class="math inline">\(M_X(t)\)</span>. La funci√≥n generatriz de momentos de <span class="math inline">\(X\)</span> es <span class="math display">\[\begin{align*}
        &amp;M_{\bar{X}}(t)=\left(M_X\left(\frac{t}{n}\right)\right)^n
        \end{align*}
\]</span> ::: {.theorem #teo6-3} <strong>Teorema (Combinaci√≥n lineal de normales es normal)</strong> (Wackerly et al.&nbsp;(2008)) Sean <span class="math inline">\(Y_1,\,Y_2,\cdots,\,Y_n\)</span> variables aleatorias independientes normalmente distribuidas <span class="math inline">\(E(Y_i)=\mu_i\)</span> y <span class="math inline">\(V(Y_i)=\sigma_i^2\)</span>ara <span class="math inline">\(i=1,\cdots,\,n\)</span> y sean <span class="math inline">\(a_1,\,a_2,\cdots,\,a_n\)</span> constantes. Si <span class="math display">\[U=\sum_{i=1}^na_iY_i\]</span></p>
<p>entonces <span class="math inline">\(U\)</span> es una variable aleatoria normalmente distribuida con <span class="math display">\[E(U)=\sum_{i=1}^na_i\mu_i\]</span> y <span class="math display">\[V(U)=\sum_{i=1}^na_i^2\sigma^2_i\]</span> :::</p>
<p><strong>Ejemplo</strong> <span class="math inline">\(X 1,\ldots, X_n\)</span> m.a.s. de <span class="math inline">\(X \sim N(\mu,\sigma^2)\)</span>. Entonces, <span class="math inline">\(M_{X}(t)=\exp\left\{\mu t+ \frac{\sigma^2t^2}{2}\right\}\)</span>. De ah√≠ que</p>
<p><span class="math display">\[
    \begin{align*}
    M_{\bar{X}}(t)
    &amp;=\left(\exp\left\{\mu \frac{t}{n}+ \frac{\sigma^2\left(\frac{t}{n}\right)^2}{2}\right\}\right)^n
    \end{align*}
\]</span></p>
<p><span class="math inline">\(X 1,\ldots, X_n\)</span> m.a.s. de <span class="math inline">\(X \sim N(\mu,\sigma^2)\)</span>. Entonces, <span class="math inline">\(M_{X}(t)=\exp\left\{\mu t+ \frac{\sigma^2t^2}{2}\right\}\)</span>. De ah√≠ que <span class="math display">\[
        \begin{align*}
        M_{\bar{X}}(t)&amp;=\exp\left\{\mu t+ \frac{\sigma^2t^2}{2n}\right\}
        \end{align*}
\]</span> De ah√≠ que <span class="math inline">\(\bar{X}\sim N(\mu,\frac{\sigma^2}{n})\)</span>.</p>
</section>
<section id="muestreo-de-una-distribuci√≥n-normal" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="muestreo-de-una-distribuci√≥n-normal"><span class="header-section-number">3.9</span> Muestreo de una distribuci√≥n normal</h2>
<section id="definici√≥n-de-distribuci√≥n-chi-cuadrada" class="level3" data-number="3.9.1">
<h3 data-number="3.9.1" class="anchored" data-anchor-id="definici√≥n-de-distribuci√≥n-chi-cuadrada"><span class="header-section-number">3.9.1</span> Definici√≥n de distribuci√≥n Chi cuadrada</h3>
<div id="def-4.10" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.1</strong></span> <strong>Definici√≥n</strong> (Wackerly et al.&nbsp;(2008)) Sea <span class="math inline">\(\nu\)</span> un entero positivo. Se dice que una v.a <span class="math inline">\(Y\)</span> tiene distribuci'on <strong>chi cuadrada con <span class="math inline">\(\nu\)</span> grados de libertad</strong> si y s√≥lo si <span class="math inline">\(Y\)</span> es una vriable aleatoria con distribuci√≥n gamma y par√°metros <span class="math inline">\(\alpha=\nu/2\)</span> y <span class="math inline">\(\beta=2\)</span>.</p>
</div>
<div id="teo-Fisher" class="theorem">
<p><strong>Teorema de Fisher</strong> En el resto del tema supondremos que <span class="math inline">\(X 1,\ldots, X_n\)</span> m.a.s. de una <span class="math inline">\(N(\mu, \sigma^2)\)</span>.</p>
<ol type="a">
<li><span class="math inline">\(\bar{X}\)</span> y <span class="math inline">\(S_n^2\)</span> son variables aleatorias independientes.</li>
<li><span class="math inline">\(\bar{X}\sim N(\mu, \frac{\sigma^2}{n})\)</span></li>
<li><span class="math inline">\(\frac{(n-1)S_n^2}{\sigma^2}\sim \mathcal{X}^2_{n-1}.\)</span></li>
</ol>
</div>
</section>
<section id="distribuciones-asociadas-a-la-normal" class="level3" data-number="3.9.2">
<h3 data-number="3.9.2" class="anchored" data-anchor-id="distribuciones-asociadas-a-la-normal"><span class="header-section-number">3.9.2</span> Distribuciones asociadas a la normal</h3>
<div id="teo6-4" class="theorem">
<p><strong>Teorema</strong> (Wackerly et al.&nbsp;(2008)) Sean <span class="math inline">\(Y_1,\,Y_2,\cdots,\,Y_n\)</span> definidas como en el Teorema 6.3 de y definimos <span class="math inline">\(Z_i\)</span> por <span class="math display">\[Z_i=\frac{Y_i-\mu_i}{\sigma_i}\]</span> con <span class="math inline">\(i=1,\,2,\cdots,\,n\)</span>. Entonces <span class="math inline">\(\sum_{i=1}^nZ_i^2\)</span> tiene distribuici'on <span class="math inline">\(\chi^2\)</span> con <span class="math inline">\(n\)</span> grados de libertad.</p>
</div>
<div id="teo7-2" class="theorem">
<p><strong>Teorema </strong> (Wackerly et al.&nbsp;(2008)) Si <span class="math inline">\(Y_1,\,Y_2,\cdots,\,Y_n\)</span> es una muestra aleatoria de una distribuci'on normal con media <span class="math inline">\(\mu\)</span> y varianza <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\(Y_i\)</span>, <span class="math inline">\(i=1,\,2,\cdots,n\)</span> son v.a‚Äôs independientes distribu'idas normalmente, con <span class="math inline">\(E(Y_i)=\mu\)</span> y <span class="math inline">\(V(Y_i)=\sigma^2\)</span>.</p>
<p>Entonces <span class="math display">\[Z_i=\frac{Y_i-\mu}{\sigma}\]</span> son v.a‚Äôs independientes, <span class="math inline">\(i=1,\,2,\cdots,n\)</span> y <span class="math display">\[\sum_{i=1}^nZ_i^2=\sum_{i=1}^n\left(\frac{Y_i-\mu}{\sigma}\right)^2\]</span>tienen una distribuci'on <span class="math inline">\(\chi^2\)</span> con <span class="math inline">\(n\)</span> grados de libertad (gl).</p>
</div>
<div id="teo7-3" class="theorem">
<p><strong>Teorema</strong> (Wackerly et al.&nbsp;(2008)) Sea <span class="math inline">\(Y_1,\,Y_2,\cdots,\,Y_n\)</span> una muestra aleatoria con media <span class="math inline">\(\mu\)</span> y varianza <span class="math inline">\(\sigma^2\)</span>. Entonces <span class="math display">\[\frac{(n-1)S^2}{\sigma^2}=\frac{1}{\sigma^2}\sum_{i=1}^n(Y_i-\overline{Y})^2\]</span> tiene una distribuci'on <span class="math inline">\(\chi^2\)</span> con <span class="math inline">\((n-1)\)</span> gl. <span class="math inline">\(\overline{Y}\)</span> y <span class="math inline">\(S^2\)</span> son v.a independientes.</p>
</div>
<div id="def7-2" class="theorem">
<p><strong>Definici√≥n</strong> (Wackerly et al.&nbsp;(2008)) Sea <span class="math inline">\(Z\)</span> una v.a normal est'andar y sea <span class="math inline">\(W\)</span> una v.a con distribuci'on <span class="math inline">\(\chi^2_\nu\)</span>. Entonces, si <span class="math inline">\(W\)</span> y <span class="math inline">\(Z\)</span> son ind <span class="math display">\[T=\frac{Z}{\sqrt{W/\nu}}\]</span> se dice que tiene una distribuci'on <span class="math inline">\(t\)</span> con <span class="math inline">\(\nu\)</span> grados de libertad.</p>
</div>
<p><strong>Observaci√≥n</strong> Si <span class="math inline">\(Y_1,\,Y_2,\cdots,\,Y_n\sim N(\mu,\sigma^2)\)</span> del Teorema @ref(teo7-1) <span class="math display">\[Z=\frac{\sqrt{n}(\overline{Y}-\mu)}{\sigma}\sim N(0,1)\]</span> El teorema @ref(teo7-3) nos dice que <span class="math display">\[W=\frac{(n-1)S^2}{\sigma^2}\sim\chi^2_{n-1}\]</span> y que <span class="math inline">\(Z\)</span> y <span class="math inline">\(W\)</span> son ind.</p>
<hr>
</section>
</section>
<section id="referencias" class="level2" data-number="3.10">
<h2 data-number="3.10" class="anchored" data-anchor-id="referencias"><span class="header-section-number">3.10</span> Referencias</h2>
<ul>
<li><p><strong>G√≥mez, Guadalupe</strong>, &amp; <strong>Delicado, Pedro</strong> (2006). <em>Curso de Inferencia y Decisi√≥n</em>. Departament d‚ÄôEstad√≠stica i Investigaci√≥ Operativa, Universitat Polit√®cnica de Catalunya.</p></li>
<li><p><strong>Wackerly, D. D., Mendenhall, W.</strong>, &amp; <strong>Scheaffer, R. L.</strong> (2008). <strong>Estad√≠stica matem√°tica con aplicaciones</strong> (7¬™ ed.). Cengage Learning.</p></li>
<li><p><strong>Roussas, G. G.</strong> (1997). <strong>A Course in Mathematical Statistics</strong> (2nd ed.). Academic Press.</p></li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>