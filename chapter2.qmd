---
title: "Estimaci√≥n puntual"
---

## La funci√≥n de distribuci√≥n emp√≠rica y el m√©todo de los momentos 


Sea la variable aleatoria $X$ con funci√≥n de distribuci√≥n $F$. Consideramos una muestra aleatoria simple de tama√±o $n$ de $X$, es decir, $X_1 ,\ldots, X_n$ v.a.i.i.d. con
distribuci√≥n dada por $F$ . Sea $x_1 ,\ldots, x_n$ una realizaci√≥n de esa m.a.s.
Se llama funci√≥n de **distribuci√≥n emp√≠rica** a la funci√≥n
$$
\begin{align}
F_{n}(x)=\dfrac{1}{n}\displaystyle\sum_{i=1}^{n}\mathbf{I}_{(-\infty,x]}(x_{i})¬∏
\end{align}
$$
que a cada n√∫mero real x le asigna la proporci√≥n de valores observados que son
menores o iguales que x.


Es inmediato comprobar que la funci√≥n $F_n$  as√≠ definida es una funci√≥n de distribuci√≥n:

1. $F_n(x) \in [0, 1]$ para todo $x \in \mathbb{R}$. 
2. $F_n$ es continua por la derecha.
3. $F_n$ es no decreciente.
4. $\lim _{x\to -\infty }F_{n}(x)=0$
5. $\lim _{x\to \infty }F_{n}(x)=1$

Concretamente, $F_n$ es la funci√≥n de distribuci√≥n de una variable aleatoria discreta (que podemos llamar $X_e$ ) que pone masa $\frac{1}{n}$ en cada uno de los n puntos
$x_i$ observados:

| $x_i$                     | $1$     | $2$     | $\ldots$ | $n$     |
|---------------------------|---------|---------|----------|---------|
| $p_i = P(X_e = x_i)$       | $1/n$   | $1/n$   | $\ldots$ | $1/n$   |


\begin{tabular}{lcccl}
	$x_i$ & 1 & 2 & $\ldots$&$n$ \\ 
	$p_i = P(X_e = x_i)$& $1/n$ & $1/n$  & $\ldots$& $1/n$
\end{tabular}

A la distribuci√≥n de $X_e$ e se le llama **distribuci√≥n emp√≠rica** asociada al conjunto de valores ${x_1 ,\ldots, x_n}$.

Obs√©rvese que si fijamos el valor de $x$ y dejamos variar la muestra, lo que obtenemos es una variable aleatoria. En efecto, se tiene entonces que

$$
\begin{align}
F_{n}(x)=\dfrac{1}{n}\displaystyle\sum_{i=1}^{n}\mathbf{I}_{(-\infty,x]}(x_{i})¬∏
\end{align}
$$

donde 

$$
\begin{align}
\mathbf{I}_{(-\infty,x]}(X_{i})= \left\{ \begin{array}{lcc}
1 &   si  & X_{i}\leq x \\
\\ 0 &  si &X_{i}> x\\
\end{array}
\right.
\end{align}
$$
y, por lo tanto, cada t√©rmino $\mathbf{I}_{(-\infty,x]}(X_{i})$ es una variable aleatoria de Bernoulli
con probabilidad de √©xito

$$
\begin{align}
p&=P(\mathbf{I}_{(-\infty,x]}(X_{i})=1)\\
&=P(X_{i}\leq x)\\
&=F(x)
\end{align}
$$
De ah√≠ se deduce que $F_n$ es una variable aleatoria y que $nF_n(x)$ tiene distribuci√≥n binomial con par√°metros $n$ y $p = F(x)$.

## Teorema de Glivenko-Cantelli

El siguiente teorema recoge algunas de las propiedades de la funci√≥n de distribuci√≥n emp√≠rica.

::: {#thm-3.1}
Sea $\{X_n\}$ para $n\geq 1$ , sucesi√≥n de variables aleatorias independientes e id√©nticamente distribuidas definidas en el espacio de probabilidad $(\Omega, \mathcal{A}, P)$ con funci√≥n de distribuci√≥n com√∫n $F$ . Se denota por $F_n$ la funci√≥n de distribuci√≥n emp√≠rica obtenida de las $n$ primeras variables aleatorias $X_1 ,\ldots, X_n$ . Sea $x\in\mathbb{R}$.
Se verifica lo siguiente:
:::

a. $P(nF_n(x)=j)=P(F_n(x)=\frac{j}{n})=\binom{n}{j}(F(x))^j(1-F(x))^{n-j}$, $j=1,\ldots,n$\\
b. $E(F_n(x))=F(x)$; $Var(F_n(x))=\frac{1}{n}F(x)(1-F(x))$.
c. $\lim _{n\to \infty }F_{n}(x)=F(x)$
d. $\lim _{n\to \infty }\frac{F_{n}(x)-F(x)}{\sqrt{\frac{F(x)(1-F(x))}{n}}}=Z$, donde $Z$ es una variable aleatoria con distribuci√≥n normal est√°ndar y la convergencia es convergencia en distribuci√≥n.

El siguiente teorema refuerza el resultado (c) anterior, puesto que afirma que la convergencia de $F_n(x)$ a $F(x)$ se da uniformemente.

::: {#thm-3.2}
Sea $\{X_n\}$ para $n\geq 1$ , sucesi√≥n de variables aleatorias independientes e id√©nticamente distribuidas definidas en el espacio de probabilidad $(\Omega, \mathcal{A}, P)$ con funci√≥n de distribuci√≥n com√∫n $F$ . Se denota por $F_n$ la funci√≥n de distribuci√≥n emp√≠rica obtenida de las $n$ primeras variables aleatorias $X_1 ,\ldots, X_n$ . Sea $x\in\mathbb{R}$. Entonces
$$Sup_{x\in\mathbb{R}} |F_{n}(x)-F(x)|\xrightarrow{c.s}0$$
:::


::: {.callout-note appearance="default" icon="false" #nte-3.1}
Obs√©rvese que seg√∫n el apartado (c) del teorema @thm-3.1, las distribuciones emp√≠ricas asociadas a muestras de tama√±o n convergen d√©bilmente a la distribuci√≥n de
probabilidad te√≥rica identificada por $F$, para casi todas las muestras de tama√±o infinito que se extraigan de $F$ . √âsta es una de las consecuencias m√°s importantes
del citado teorema:
la distribuci√≥n emp√≠rica converge d√©bilmente con probabilidad 1 a la poblacional cuando el tama√±o de la muestra tiende a infinito:
	$$F_{n}(x)\xrightarrow{c.s}F(x)$$
Esto garantiza la posibilidad de realizar inferencia estad√≠stica:

1. Los aspectos probabil√≠sticos de una caracter√≠stica $X$, medida en una poblaci√≥n, se resumen de
forma estilizada en una distribuci√≥n de probabilidad $F$.
2. La distribuci√≥n de probabilidad $F$, puede ser aproximada mediante las distribuciones emp√≠ricas $F_n$ obtenidas por muestreo de la poblaci√≥n en estudio.
3. El teorema de Glivenko-Cantelli afirma que esas aproximaciones son uniformes en x.
4. Por esta raz√≥n el teorema de Glivenko-Cantelli
se llama a veces Teorema Fundamental de la Estad√≠stica Matem√°tica.
::: 

**Podemos ver a continuaci√≥n c√≥mo, a medida que aumentamos el tama√±o de la muestra (n=10,30,100,1000), la funci√≥n de distribuci√≥n emp√≠rica se ajusta cada vez mejor a la distribuci√≥n te√≥rica normal est√°ndar N(0,1), tal como afirma el teorema.**

```{r}
# Cargar librer√≠a para gr√°ficos
library(ggplot2)

# Definir funci√≥n para graficar ECDF vs distribuci√≥n te√≥rica
comparar_ecdf_teorica <- function(n, distribucion = "normal") {
  set.seed(123)  # Para reproducibilidad

  # Muestra de tama√±o n desde N(0,1)
  muestra <- rnorm(n)
  
  # Dominio com√∫n
  x_vals <- seq(-3, 3, length.out = 1000)

  # Distribuci√≥n te√≥rica
  F_teorica <- pnorm(x_vals)

  # Distribuci√≥n emp√≠rica
  ecdf_muestra <- ecdf(muestra)
  F_empirica <- ecdf_muestra(x_vals)

  # Construir data frame para ggplot
  df <- data.frame(
    x = rep(x_vals, 2),
    F = c(F_empirica, F_teorica),
    Tipo = rep(c("Emp√≠rica", "Te√≥rica"), each = length(x_vals))
  )

  # Graficar
  ggplot(df, aes(x = x, y = F, color = Tipo, linetype = Tipo)) +
    geom_line(size = 1) +
    labs(
      title = paste("ECDF vs F(x) ‚Äî Tama√±o de muestra n =", n),
      x = "x", y = "Probabilidad acumulada"
    ) +
    theme_minimal() +
    scale_color_manual(values = c("Emp√≠rica" = "red", "Te√≥rica" = "black")) +
    scale_linetype_manual(values = c("Emp√≠rica" = "dashed", "Te√≥rica" = "solid"))
}

# Generar gr√°ficos para diferentes tama√±os de muestra
comparar_ecdf_teorica(10)
comparar_ecdf_teorica(30)
comparar_ecdf_teorica(100)
comparar_ecdf_teorica(1000)

```


**El Teorema Fundamental de la Estad√≠stica Matem√°tica: da una fundamentaci√≥n de la inferencia estad√≠stica, cuyo objetivo principal consiste en extraer informaci√≥n sobre $F$ a partir de las observaciones muestrales.**

**¬øPor qu√© esto es importante?**

Porque sin conocer $F(x)$ expl√≠citamente, **podemos estimarla a partir de los datos**.  
Esto es la base de:

- los **histogramas acumulados**,
- las **pruebas no param√©tricas**,
- los **intervalos de confianza emp√≠ricos**, y
- toda la **inferencia estad√≠stica basada en datos reales**.


üéØ **Ejemplo: Estimaci√≥n del percentil 90 del ingreso mensual**

üìå Contexto
Sup√≥n que quieres estimar el ingreso mensual **por debajo del cual se encuentran el 90% de las personas** en una ciudad.  
No conoces la distribuci√≥n real del ingreso \( F(x) \), pero tienes una muestra de datos.

---

**Paso a paso**

1. Simulamos una poblaci√≥n
Vamos a suponer que el ingreso sigue una distribuci√≥n log-normal:

```{r}
set.seed(123)
poblacion <- rlnorm(1e6, meanlog = 10, sdlog = 0.5)
```

2. Tomamos una muestra aleatoria

```{r}
muestra <- sample(poblacion, size = 20000, replace = FALSE)
```

3. Estimamos la distribuci√≥n emp√≠rica
```{r}
Fn <- ecdf(muestra)
```

4. Estimamos el percentil 90 (cuantil 0.9)

```{r}
cuantil_90_empirico <- quantile(muestra, probs = 0.9)
```

5. Comparamos con el valor verdadero en la poblaci√≥n
```{r}
cuantil_90_real <- quantile(poblacion, probs = 0.9)
```

6. Resultado

```{r}
cat("Cuantil 90 estimado (emp√≠rico):", round(cuantil_90_empirico, 2), "\n")
cat("Cuantil 90 real (poblacional):", round(cuantil_90_real, 2), "\n")
```

## La funci√≥n de distribuci√≥n emp√≠rica y el m√©todo de los momentos 
### Principio de sustituci√≥n
En esta secci√≥n presentamos una consecuencia importante de la convergencia de $F_n$ a $F$ , la definici√≥n de estimadores mediante el principio de sustituci√≥n.


a. La convergencia de $F_n$ a $F$ permite construir versiones factibles de caracter√≠sticas poblacionales desconocidas.
b. Supongamos que estudiamos una caracter√≠stica $X$ en una poblaci√≥n y que el resultado de la observaci√≥n de $X$ puede ser modelado como una variable aleatoria con distribuci√≥n desconocida, digamos $F$.
c. Muchas de las preguntas relevantes acerca de la caracter√≠stica $X$ podr√≠an ser contestadas si su funci√≥n de distribuci√≥n $F$ fuese conocida.


::: {.callout-important appearance="default" icon="false" #imp-3.1}
**Preguntas sobre $X$**

- el valor esperado,
- el n√∫mero de modas de la distribuci√≥n o 
- la probabilidad de que $X$ sea negativa


Para fijar ideas podemos pensar que nos interesa conocer cantidades num√©ricas (par√°metros) que dependen √∫nicamente de la funci√≥n de distribuci√≥n desconocida $F$:

$$
	\begin{align}
	\theta=\psi(F)
	\end{align}
$$
**El teorema de Glivenko-Cantelli** nos dice que $F_n$ se acerca a $F$, a medida que el tama√±o muestral crece. As√≠, podemos esperar que tambi√©n se verifique que
$$
	\begin{align}
	\hat{\theta}_n=\psi(F_n)\rightarrow\theta=\psi(F)
	\end{align}
	$$


Es decir, esperamos que las cantidades num√©ricas calculadas para la distribuci√≥n emp√≠rica (estimadores) se aproximen a las cantidades desconocidas a medida que el tama√±o muestral crezca.

Esta forma de obtener estimadores de par√°metros poblacionales desconocidos se denomina principio de sustituci√≥n (plug-in principle en ingl√©s). Es un procedimiento muy general de obtenci√≥n de estimadores.
::: 


Sea $X\sim U(0,\theta)$. Se toma una m.a.s. de $X$ de tama√±o n para estimar $\theta$. Un estimador razonable de $\theta$ es el m√°ximo de las observaciones, que es estad√≠stico
minimal suficiente para $\theta$:
$$
\begin{align}
	\hat{\theta}_2 = \max_i {X_i}.
\end{align}
$$
El siguiente c√≥digo muestra:

- Para cada tama√±o de muestra n, simula valores de $X_i$‚àºU(0,Œ∏),

- Calcula $$\hat{\theta} = \max_i {X_i}$$
- Compara con el valor real de $\theta=10$.

- Muestra c√≥mo, al aumentar n, el estimador se acerca a $\theta$.

```{r}
# Simulaci√≥n para estimar theta en una uniforme (0, theta)
set.seed(42)
theta_real <- 10

# Tama√±os de muestra
n_vals <- c(5, 10, 30, 100)

# Simular y comparar
estimadores <- sapply(n_vals, function(n) {
  muestra <- runif(n, min = 0, max = theta_real)
  max(muestra)
})

# Mostrar resultados
data.frame(
  Tama√±o_muestra = n_vals,
  Estimador_maximo = round(estimadores, 3),
  Error = round(theta_real - estimadores, 3)
)
```


**Convergencia del estimador plug-in en la distribuci√≥n uniforme**

En este ejemplo, estimamos el par√°metro $\theta$ de una distribuci√≥n $X \sim \mathcal{U}(0, \theta)$ usando el estimador $\hat{\theta}_n = \max(X_i)$. Este es un estimador tipo plug-in: se usa la distribuci√≥n emp√≠rica para estimar una caracter√≠stica de la distribuci√≥n te√≥rica.

A continuaci√≥n, simulamos c√≥mo este estimador converge al verdadero valor $\theta = 10$ a medida que el tama√±o de muestra $n$ crece.

```{r}
# Chunk de R ‚Äî solo c√≥digo
#| label: fig-convergencia-max
#| fig-cap: 'Convergencia del estimador plug-in $ \hat{\theta}_n = \max X_i $ hacia el valor real $ \theta = 10 $'
#| fig-align: center
#| message: false
#| warning: false

set.seed(42)
theta_real <- 10

# Vector de tama√±os de muestra crecientes
n_seq <- seq(5, 500, by = 5)

# Calcular el estimador para cada n
estimadores <- sapply(n_seq, function(n) {
  muestra <- runif(n, min = 0, max = theta_real)
  max(muestra)
})

# Crear data frame para graficar
df_estimacion <- data.frame(
  n = n_seq,
  estimador = estimadores
)

# Graficar
library(ggplot2)
ggplot(df_estimacion, aes(x = n, y = estimador)) +
  geom_line(color = "steelblue", size = 1) +
  geom_hline(yintercept = theta_real, color = "red", linetype = "dashed") +
  labs(
    title = expression("Convergencia de " * hat(theta)[n] * " al valor real " * theta),
    x = "Tama√±o de muestra (n)",
    y = expression(hat(theta)[n])
  ) +
  theme_minimal()
```


## M√©todo de momentos

Una aplicaci√≥n del principio de sustituci√≥n es la definici√≥n de los estimadores basados en momentos. El momento **no centrado** de orden $k$ de una variable aleatoria $X$ con distribuci√≥n $F$ se define como
$$
\begin{align}
\mu_k=E_F(X^k)=\int x^kdF(x)
	\end{align}
$$
Si $X_e$ es una variable aleatoria con funci√≥n de distribuci√≥n igual a $F_n$ , la funci√≥n de distribuci√≥n emp√≠rica de una m.a.s. de tama√±o $n$ de $X$, se tiene que sus \textcolor{red}{momentos no centrados} (a los que llamaremos $m_{k,n}$) son de la forma
	\begin{align}
	m_{k,n}=E_{F_n}(X_e^k)=\int x^kdF_n(x)=\frac{1}{n}\sum_{i=1}^{n}X_i^k,
	\end{align}
	y se denominan momentos muestrales no centrados de orden $k$. Por ejemplo, $¬µ_1$ es la esperanza poblacional y $m_{1,n}$ la media muestral.


La siguiente proposici√≥n garantiza que los momentos muestrales convergen a los poblacionales.
    
::: {#prp-3.1}
Sea $X$ variable aleatoria con $E(X^{2k}) < \infty$. Entonces se verifica que $m_{k,n} \rightarrow \mu_k$ casi seguro. Adem√°s,
$$
\begin{align}
\frac{\sqrt{n}(m_{k,n}-\mu_k)}{\sqrt{\mu_{2k}-\mu_k^2}}\xrightarrow{d}Z,
\end{align}
$$
con $Z\sim N(0,1)$. 
::: 

::: {.proof}
Si $Y_i=X_i^k$ entonces  $m_{k,n}=E_{k,n}(Y_i)=E_{n}(X_i^k)=\frac{1}{n}\sum_{i=1}^{n}X_i^k=\frac{1}{n}\sum_{i=1}^{n}Y_i=\bar{Y}_n$.\\

Aplicando la ley fuerte de los grandes n√∫meros se tiene que 
\begin{align}
\lim _{n\to \infty }\frac{S_n-E(S_n)}{n}&=\lim _{n\to \infty }\frac{\sum_{i=1}^{n}X_i^k-E(\sum_{i=1}^{n}X_i^k)}{n}\nonumber\\
&=\lim _{n\to \infty }\left[\frac{\sum_{i=1}^{n}X_i^k}{n}-\frac{E(\sum_{i=1}^{n}X_i^k)}{n}\right]\nonumber\\
&=\lim _{n\to \infty }\left[\bar{Y}_n-\frac{\sum_{i=1}^{n}E(X_i^k)}{n}\right]\nonumber\\
&=\lim _{n\to \infty }\left[\bar{Y}_n-\frac{nE(X^k)}{n}\right]\mbox{Por ser las $X_i$ una mas de X}\nonumber\\
&=\lim _{n\to \infty }\left[\bar{Y}_n-\bar{Y}\right]=0   \mbox{ Aplicando L.F.G.N}
\end{align}

Por lo anterior, se tiene que
 $m_{k,n}=\bar{Y}_n\xrightarrow{c.s} \mu_k=\bar{Y}=E_F(X^k)$. Por otro lado, veamos 
 \begin{align}
\frac{S_n-E(S_n)}{\sqrt{Var(S_n)}}=\frac{m_{k,n}-E(X^k)}{\sqrt{\frac{Var(X^k)}{n}}}\xrightarrow{d}Z
 \end{align}
 donde $S_n=\sum_{i=1}^{n}X_i^k$. Sabemos que $E(X^k)=\mu_k$ y que $Var(X^k)=E[(X^k)^2]-[E(X^k)]^2=\mu_{2k}-\mu_k^2$, de ah√≠ se sigue el resultado.
:::

Muchas caracter√≠sticas poblacionales de inter√©s se pueden expresar como funci√≥n de los momentos no centrados de √≥rdenes $1,\ldots, k$: $\theta=h(\mu_1, \ldots, \mu_k)$. Por ejemplo, la varianza de $X$ se expresa como $\sigma^2 = h(\mu_1, \mu_2) = \mu_2-\mu_1^2$.

**El estimador de $\theta$ basado en el principio de sustituci√≥n se conoce como estimador de los momentos de $\theta$ y ser√°
	\begin{align}
\hat{\theta}_n = h(m_{1,n} ,\ldots, m_{k,n}).
	\end{align}
	Obs√©rvese que el estimador de los momentos de $\theta$ puede no ser √∫nico, porque diferentes funciones $h$ pueden conducir al mismo valor $\theta$.**

::: {#prp-3.2}
Consideremos la variable aleatoria $X$ con $E(X_{2k})< \infty$. Sea $\theta=h(\mu_{n} ,\ldots, \mu_{n})$. Si $h$ es continua en $(\mu_{n} ,\ldots, \mu_{n})$, entonces $\hat{\theta}_n=h(m_{1,n} ,\ldots, m_{k,n})$ converge a $\theta$ casi seguro. Adem√°s, si $h$ es derivable en $(\mu_{n} ,\ldots, \mu_{n})$, entonces la
distribuci√≥n l√≠mite de $\hat{\theta}_n$ es normal:
\begin{align}
\sqrt{n}(\hat{\theta}_n-\theta)\xrightarrow{d} N(0, \sigma_{h,\theta}^2)
\end{align}
::: 


::: {#exm-3.1}
Sea $X\sim U(0,\theta)$. Se toma una m.a.s. de $X$ de tama√±o n para estimar $\theta$. Un estimador de momentos  $\hat{\theta}_M$ de $\theta$ viene dado por la siguiente relaci√≥n 
	\begin{align}
	E(X)=\frac{\theta}{2}\Longrightarrow m_{1,n}=\frac{\hat{\theta}_M}{2}\Longrightarrow 2m_{1,n}=\hat{\theta}_M\Longrightarrow 2\bar{X}=\hat{\theta}_M
	\end{align}
::: 


::: {#exm-3.2}
\item[1.] Para la variable aleatoria $X$ con varianza finita, un estimador para $\theta=Var(X)$ es 
\begin{align}
\hat{\theta}=h(m_{1,n}, m_{2,n})&=m_{2,n}-m_{1,n}^2\nonumber\\
&=\frac{1}{n}\sum_{i=1}^{n}x_i^2-\bar{x}^2\nonumber\\
&=\frac{\sum_{i=1}^{n}x_i^2-n\bar{x}^2}{n}\nonumber\\
&=\frac{\sum_{i=1}^{n}(x_i-\bar{x})^2}{n}\nonumber\\
&=\frac{(n-1)S_n^2}{n}\nonumber\\
\end{align}	

::: 

::: {#exm-3.3}
\item[2.]Si $X\sim Exp(\lambda)$ con $E(X)=\frac{1}{\lambda}$, entonces $m_{1,n}=\frac{1}{\hat{\lambda}_M}$ $\Longrightarrow \hat{\lambda}_M=\frac{1}{m_{1,n}}\Longrightarrow \hat{\lambda}_M=\frac{1}{\bar{X}}$.
\item[3.]Si $X\sim B(n,p)$, con $E(X)=np$ y $Var(X)=npq$, entonces $m_{1,n}=n\hat{p}$ $\Longrightarrow$ $\frac{m_{1,n}}{n}=\hat{p}$ $\Longrightarrow$ $\frac{\bar{X}}{n}=\hat{p}$ $\Longrightarrow$ $\hat{Var(X)}=n\hat{p}(1-\hat{p})$.
::: 


