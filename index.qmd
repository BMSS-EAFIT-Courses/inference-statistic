---
# Si quieres un encabezado local, puedes dejarlo así:
# O simplemente eliminar este bloque completamente
---

# INTRODUCCIÓN
Este libro ha sido concebido como un recurso integral para el estudio riguroso y aplicado de la inferencia estadística. Está dirigido a estudiantes de programas de estadística, matemáticas aplicadas y disciplinas afines, y busca fortalecer la comprensión conceptual y técnica de los fundamentos que sustentan el análisis estadístico moderno.

A lo largo de sus capítulos, el lector encontrará un desarrollo progresivo de los siguientes temas:

- Fundamentos de la probabilidad y los modelos estadísticos.
- Propiedades de las variables aleatorias y familias paramétricas comunes.
- Principios clave para la reducción de datos: suficiencia, completitud y verosimilitud.
- Construcción y evaluación de estimadores puntuales.
- Estimación por intervalos y su interpretación inferencial.
- Contrastes de hipótesis y principios de optimalidad en pruebas estadísticas.
- Introducción a la teoría de la decisión y sus aplicaciones en inferencia.

El material combina el rigor formal con ejemplos y aplicaciones que ilustran cómo los métodos estadísticos permiten extraer conclusiones válidas a partir de datos.

> Este libro se encuentra en construcción. Los capítulos se irán publicando progresivamente y pueden estar sujetos a revisiones o mejoras. Por ahora, son sólo trozos de contenido de otros libros o notas de clase de una selección personal y que se irán referenciando en cada capítulo.

---

## ¿Cómo navegar este libro?

- Usa el **índice lateral izquierdo** para acceder a cada capítulo y subcapítulo.
- Haz uso del **buscador** para encontrar conceptos o términos clave.
- Revisa los apartados de “Lista de problemas” incluidos al final de cada sección para practicar.

---

## ¡Bienvenida/o!

Te invito a recorrer este texto con atención, curiosidad y sentido crítico.  
Espero que este libro te acompañe, rete y apoye en tu formación como profesional en ciencias de datos o áreas relacionadas.

<!-- 1. [Datos y modelos](Datos%20y%20modelos.qmd)   -->
<!-- 2. [Variable aleatoria](Variable%20aleatoria.qmd)   -->




# DATOS Y MODELOS

En esta sección establecemos la base conceptual para el análisis estadístico, diferenciando claramente entre el fenómeno aleatorio observado y la medida de probabilidad que lo describe.  

## Fenómeno aleatorio y variable observada

> “Se observa una realización de un fenómeno aleatorio, digamos X. Este puede ser un elemento aleatorio de varios tipos: número (variable aleatoria), un vector de dimensión finita (vector aleatorio), una función, etc.  
> La premisa principal es que el carácter aleatorio de X se concibe como una realización de un fenómeno aleatorio que tiene una distribución de probabilidad P, donde la distribución P es desconocida ya sea en su totalidad o en algún detalle específico (por ejemplo, su soporte, su media, etc.). Es de interés conocer P. Si la medida de probabilidad P fuese conocida, entonces no hay problema estadístico propiamente, pues el problema estadístico tiene que ver con _inferir_ la propiedad desconocida de P con base en X.” [Ver referencia 1]

- **Definición de X**  
  - **X** puede ser un valor real $X \in \mathbb{R}$, un vector en $\mathbb{R}^n$, o incluso una función $\;X: [0,1]\to\mathbb{R}$.  
- **Medida de probabilidad P**  
  - Desconocida: soporte, media, varianza, etc.  
  - Objetivo estadístico: _inferir_ características de \(P\) a partir de la muestra (la realización de X).

## Incertidumbre inductiva vs. estocástica

> “La observación **X** está dada, por lo que no hay incertidumbre tal como la hay en la teoría de probabilidad desarrollada anteriormente en el curso. Antes, fue concebida una estructura $(\Omega, \mathcal{F}, P)$ para enfrentar el que haya incertidumbre acerca del valor de **X**. En el problema estadístico, el valor de **X** ha sido observado, y la incertidumbre radica en otro punto: radica en que existe duda acerca de cuál **P** es la que produjo el valor **X**. En algunas ocasiones se utilizan los términos _incertidumbre estocástica_ e _incertidumbre inductiva_ para distinguir estos dos tipos. Es común que estos se confundan entre sí, porque en estadística matemática la teoría de probabilidad constituye también una de las maneras naturales de afrontar la cuantificación de incertidumbre inductiva. En cualquier caso, el concebir a **P** como medida de probabilidad es la base para formular soluciones a la incertidumbre inductiva. Con este lenguaje, probabilidad y estadística son problemas diferentes y de cierta manera inversos. Teoría de probabilidad tiene que ver con cuantificar incertidumbre acerca de **X** y teoría estadística con cuantificar incertidumbre acerca de **P** a la luz de haber ya observado **X**.”[Ver referencia 1]

- **Incertidumbre estocástica**: duda previa sobre el valor de X, modelada por $(\Omega,\mathcal{F},P)$.  
- **Incertidumbre inductiva**: tras observar X, la incertidumbre se desplaza a la ley generadora P.

---

#### Ejemplos en Matemática Aplicada e Ingeniería de Sistemas

1. **Modelado de tiempos de respuesta en redes**  
   - $X$: tiempo de llegada de paquetes (variable continua).  
   - $P$: distribución de retardo desconocida; objetivo: estimar parámetros de una ley de colas M/M/1.  

2. **Estimación de parámetros en ecuaciones diferenciales estocásticas**  
   - $X(t)$: trayectoria observada de un proceso de Itô.  
   - $P$: ley del proceso (por ejemplo, coeficientes de difusión y deriva), inferidos a partir de trayectorias discretas.  

3. **Calibración de sensores en sistemas de control**  
   - $X$: lecturas del sensor (vector aleatorio).  
   - $P$: distribución conjunta desconocida de ruido; se estima para diseñar filtros de Kalman óptimos.  

---

Con esta distinción clara entre dato observado y modelo probabilístico, estamos listos para construir estimadores y desarrollar la inferencia estadística en las secciones siguientes. 




# VARIABLE ALEATORIA


## Variables y vectores aleatorios
Consideramos un experimento aleatorio cuyos resultados pertenecen al espacio muestral Ω. Modelamos este proceso suponiendo que existe una terna $(\Omega, \mathcal{A}, P),$ donde:

- $\Omega$ es el espacio muestra,  
- $\mathcal{P}(\Omega)$ es el conjunto de partes de Ω,  
- $\mathcal{A}\in\mathcal{P}(\Omega)$ es una σ-álgebra,  
- $P\colon \mathcal{A} \to [0,1]$ es una medida de probabilidad que refleja las características aleatorias del experimento realizado.  

A esta terna se le llama **espacio de probabilidad**.

Los resultados de un experimento aleatorio no son analizados “en bruto”, sino que se les da una representación numérica que facilita su tratamiento. Esto se logra introduciendo variables aleatorias, que asocian cada resultado $\omega\in \Omega$ con un valor numérico o vectorial, y sobre las cuales luego aplicamos técnicas de inferencia estadística.

En todo estudio estadístico partimos de un **experimento aleatorio** cuyo conjunto de resultados posibles se denomina **espacio muestral** Ω. Para cuantificar dichos resultados definimos las siguientes estructuras:

**Definición: Variables Aleatorias**

Sea $(\Omega,\mathcal{A},P)$ un espacio de probabilidad. Una **variable aleatoria** es una función $X\colon (\Omega,\mathcal{A})\;\longrightarrow\; (\mathbb{R},\mathcal{B}),$ tal que para todo $B\in\mathcal{B}$ (la $\sigma$-álgebra de Borel en ℝ),  $X^{-1}(B)\;=\;\{\omega\in\Omega : X(\omega)\in B\}\;\in\;\mathcal{A}.$

- Si el espacio muestral Ω es **finito o numerable**, diremos que es un espacio **discreto** y las variables aleatorias asociadas al experimento normalmente estarán definidas como $X\colon \Omega \;\longrightarrow\; \mathbb{Z}.$

- Si $\Omega$ es **no numerable**, entonces diremos que es un espacio **continuo** y $X\colon \Omega \;\longrightarrow\; \mathbb{R}.$
---

**Definición: Vector aleatorio**

Un **vector aleatorio** de dimensión $n$ es $\mathbf{X} = (X_1,\dots,X_n)\colon(\Omega,\mathcal{A})\longrightarrow(\mathbb{R}^n,\mathcal{B}^n),$ donde cada componente $X_i$ es variable aleatoria y $\mathcal{B}^n$ la $\sigma$-álgebra de Borel en ℝⁿ.

---

**Ejemplos**
  **Lanzamiento de dos monedas**

Sea $\Omega =\{\,CC,\;C-,\;-C,\;--\},$ donde $C$ = “cara” y $-$ = “cruz”. Podemos definir:  
$X_1(\omega) = \text{número de caras en }\omega.$
$X_2(\omega) = 2 - X_1(\omega)\;=\; \text{número de cruces}.$
$X_3(\omega) = \bigl(X_1(\omega)\bigr)^2.$

Entonces $(X_1,X_2,X_3)$ es un vector aleatorio de dimensión 3.

 **Tiempos de servicio en un servidor**

Sean $T_i$ los tiempos de servicio (en segundos) de las peticiones $i=1,2,3$. Definimos  
$\mathbf{T}=(T_1,T_2,T_3),\quad S = T_1 + T_2 + T_3,\quad M = \max\{T_1,T_2,T_3\}.$

  **Lecturas de sensores en red distribuida**

En tres nodos $i=1,2,3$ medimos temperatura $X_{i,1}$, presión $X_{i,2}$ y humedad $X_{i,3}$. El vector global es $\mathbf{X} = (X_{1,1},X_{1,2},X_{1,3},\,X_{2,1},\dots,X_{3,3}) \in \mathbb{R}^9.$

---

Con estas definiciones rigurosas disponemos ya de los objetos básicos para, en las siguientes secciones, construir estimadores, estudiar su comportamiento asintótico y contrastar hipótesis sobre la distribución subyacente $P$.


## Distribución de una variable aleatoria. Funciones de distribución, de probabilidad y de densidad
  
  **Distribución de una Variable Aleatoria**

La realización de un experimento aleatorio da lugar a un resultado $\omega\in\Omega$ que es aleatorio. Por lo tanto, $X(\omega)$ es un valor de $\mathbb{R}$ también aleatorio. Es decir, la variable aleatoria $X$ induce una medida de probabilidad en $\mathbb{R}$. A esa medida de probabilidad se le llama **distribución de $X$** o **ley de $X$**. Una de las formas de caracterizar la distribución de una variable aleatoria es dar su función de distribución $F_X$, que está definida así:

$F_X(x) \;=\; P(X \le x)\;=\; P\bigl(\{\omega \in \Omega : X(\omega) \le x\}\bigr)\;=\; P\bigl(X^{-1}((-\infty, x])\bigr).$$

En el caso de que $X$ sea una **variable aleatoria discreta**, es decir, en el caso de que $X$ solo tome una cantidad finita o numerable de valores de $\mathbb{R}$, su distribución también puede caracterizarse por su **función de probabilidad** (o **función de masa de probabilidad**) $f_X$, definida como

$$f_X : \mathbb{R} \longrightarrow [0,1],\qquad f_X(x) = P(X = x).$$

Esa función solo es no nula en un conjunto finito o numerable. Supondremos en adelante, sin pérdida de generalidad, que ese conjunto está contenido en $\mathbb{Z}$.
A partir de la función de masa de probabilidad se puede calcular la probabilidad 
de que la variable aleatoria $X$ tome valores en cualquier elemento $A \subseteq \mathbb{B}$:

$P(X \in A) = \sum_{x \in A} f_X(x).$
me


La función de distribución y la función de masa de probabilidad se relacionan 
de la siguiente forma:

$F_X(x) = \sum_{u \leq x} f_X(u), \quad f_X(x) = F_X(x) - F_X(x^-),$ donde $F_X(x^-) = \lim_{h \to 0^+} F_X(x - h)$.

Una clase relevante de variables aleatorias no discretas son las que poseen **función de densidad**, es decir, aquellas cuya distribución de probabilidad puede caracterizarse por una función $f_X(x) \geq 0$ que cumple que:

$P(X \in A) = \int_{x \in A} f_X(x) \, dx, \quad \text{para todo } A \subseteq \mathbb{B}.$

La relación entre $F_X$ y $f_X$ es la siguiente:

$F_X(x) = \int_{-\infty}^{x} f_X(u) \, du, \quad f_X(x) = \frac{d}{dx} F_X(x),$

salvo quizás en un número finito de puntos $x \in \mathbb{R}$. Las variables aleatorias que poseen función de densidad se llaman **variables aleatorias absolutamente continuas**. Abusando del lenguaje, aquí nos referiremos a ellas como variables aleatorias continuas.



## Esperanza y varianza
Si se desea describir totalmente la distribución de probabilidad de una variable aleatoria $X$ acabamos de ver que podemos dar su función de distribución o su función de masa o de densidad, según el caso. Una descripción parcial puede efectuarse calculando algunas características de la variable aleatoria $X$, como por ejemplo medidas de posición o de dispersión. Estudiaremos algunas de ellas.

Se define la **esperanza** de una variable aleatoria $X$ como la integral de Lebesgue de $X$:

$E(X) = \int_{\Omega} X(w) dP(w).$

En el caso de variables aleatorias discretas la esperanza puede calcularse como:

$E(X) = \sum_{w \in \Omega} X(w) P(w) = \sum_{k \in \mathbb{Z}} k P(X = k) = \sum_{k \in \mathbb{Z}} k f_X(k).$

Por otro lado, la esperanza de una variable aleatoria continua se puede calcular así:

$E(X) = \int_{\mathbb{R}} x f_X(x) dx.$

La esperanza de una variable aleatoria $X$ es una medida de posición de $X$: es el centro de gravedad de la distribución de probabilidad de $X$.

Si $h$ es una función medible $h : \mathbb{R} \rightarrow \mathbb{R}$, entonces $Y = h(X)$ es también variable aleatoria y su esperanza se puede calcular a partir de la distribución de $X$:

$E(h(X)) = \int_{\Omega} h(X(w)) dP(w)$ que en el caso de que $X$ sea discreta puede reescribirse como

$E(h(X)) = \sum_{k \in \mathbb{Z}} h(k) f_X(k).$

Si $X$ es una variable aleatoria continua entonces

$E(h(X)) = \int_{\mathbb{R}} h(x) f_X(x) dx.$

Si existe $\mu = E(X)$ y es finita puede definirse una medida de dispersión de la variable aleatoria $X$ a partir de una transformación $h$ de $X$. Es lo que se denomina **varianza** de $X$ y se define así:

$V(X) = E((X - \mu)^2) = E(X^2) - \mu^2 = E(X^2) - (E(X))^2.$

## Función generadora de momentos 
Dada una variable aleatoria $X$, o su función de distribución $F$, vamos a definir otra función generadora, como

$M_X(t) = \mathbb{E}(e^{tX}),$ siempre que este valor esperado exista.

Notemos que cuando $X$ toma valores en los enteros no-negativos, $M_X(t) = \phi_X(e^t)$, donde $\phi_X(s)=E[s^X]=\sum_{k=0}^{\infty}p_ks^k$ para $s\in[0,1]$ es la función generadora de probabilidad (f.g.p.) de la variable $X$, con $p_k=P(X=k)$. Si $X$ está acotada, $M_X$ está bien definida para todo $t$ real; en cambio, si $X$ no está acotada, es posible que el dominio de $M_X$ no sea el conjunto de todos los reales. En todo caso, $\phi$ siempre está definida en cero, y $M(0) = 1$.

Es posible demostrar que si la f.g.m. de la v.a. $X$ existe en un entorno de 0, entonces para todo $k > 0$,

$\mathbb{E}[|X|^k] < \infty.$

Más aún, la serie

$M_X(t) = 
\mathbb{E}(e^{tX}) 
= \mathbb{E}\left(1 + \sum_{k=1}^{\infty} \frac{t^k X^k}{k!}\right) 
= 1 + \sum_{n=1}^{\infty} \frac{t^k}{k!} \mathbb{E}(X^k)
\tag{5.1}$

es convergente y se puede derivar término a término. Obtenemos

$M'_X(0) = \mathbb{E}(X); \quad M''_X(0) = \mathbb{E}(X^2)$

y en general

$M_X^{(k)}(0) = \mathbb{E}(X^k).$

Es por esta última propiedad que esta función se conoce como **función generadora de momentos** (f.g.m.).

🎲 Ejemplo: f.g.m. de la distribución Binomial

Sea $X \sim \text{Binomial}(n, p)$, es decir, la suma de $n$ ensayos de Bernoulli con probabilidad de éxito $p$. La función generadora de momentos es: [Ejemplo fgm binomial](https://github.com/BMSS-EAFIT-Courses/inference-statistic/blob/main/mgf_binomial.r)

$M_X(t) = \mathbb{E}[e^{tX}] = (1 - p + p e^t)^n$

<pre> ```r #
mgf_binomial <- function(t, n = 10, p = 0.3) {
  (1 - p + p * exp(t))^n
}

t_vals <- seq(-3, 3, length.out = 300)
mgf_vals <- sapply(t_vals, mgf_binomial)

plot(t_vals, mgf_vals, type = "l", lwd = 2,
     main = expression("F.G.M. para X ~ Binomial(10, 0.3)"),
     xlab = "t", ylab = expression(M[X](t)))
grid()
``` </pre>
📈 Ejemplo: f.g.m. de la distribución Normal Estándar

Sea $X \sim \mathcal{N}(0, 1)$. Su función generadora de momentos es:

$M_X(t) = \mathbb{E}[e^{tX}] = e^{\frac{t^2}{2}}$

Esta expresión se obtiene usando la forma cerrada del momento de una normal estándar.
<pre> ```r #
mgf_normal <- function(t) {
  exp(t^2 / 2)
}

t_vals <- seq(-3, 3, length.out = 300)
mgf_vals <- sapply(t_vals, mgf_normal)

plot(t_vals, mgf_vals, type = "l", lwd = 2,
     main = expression("F.G.M. para X ~ N(0, 1)"),
     xlab = "t", ylab = expression(M[X](t)))
grid()
``` </pre>

 **❓ Preguntas guía sobre la gráfica de la función generadora de momentos**

  **📌 ¿Qué representa la gráfica de la f.g.m. $M_X(t)$?**

La gráfica muestra cómo evoluciona el valor esperado de $e^{tX}$ cuando $t$ varía. Esta función codifica **todos los momentos de la variable aleatoria** $X$, y por tanto, contiene información completa sobre su distribución (si existe un entorno donde la f.g.m. es finita).

---

  **🧭 ¿Qué se observa en la f.g.m. de una distribución Binomial?**
<img width="480" height="480" alt="mgf_binomial" src="https://github.com/user-attachments/assets/8524edee-0b5c-4938-a9e8-8af0e8c5840c" />

![Gráfica MGF Binomial]

  #Preguntas y respuestas

- **¿Cómo es el comportamiento de la f.g.m. cerca de $t = 0$?**

  En $t = 0$, siempre se cumple que $M_X(0) = 1$, ya que:

  $M_X(0) = \mathbb{E}[e^{0 \cdot X}] = \mathbb{E}[1] = 1$

- **¿Qué indica la curvatura de la gráfica?**

   La curvatura refleja el crecimiento exponencial de los momentos. Si la curva crece rápidamente hacia la derecha, significa que los momentos (media, varianza, etc.) también crecen con rapidez.

- **¿Por qué la gráfica es convexa?**

  Todas las funciones generadoras de momentos son **estrictamente convexas** en el intervalo donde están definidas. Esto es una consecuencia de que derivadas sucesivas representan momentos positivos.

- **¿Qué pasa si cambio los parámetros $n$ y $p$?**

  Aumentar $ n $ o $p$ tiende a **elevar** la f.g.m. en el lado derecho, reflejando una mayor media y varianza.

---

  **📈 ¿Cómo se comporta la f.g.m. para la Normal Estándar?**
<img width="480" height="480" alt="mgf_normal" src="https://github.com/user-attachments/assets/e877129d-4235-42d9-baff-7f29d3afc4a7" />

![Gráfica MGF Normal]

  **Preguntas y respuestas**

- **¿Por qué es simétrica respecto al eje $ t = 0 $?**

  Porque la normal estándar es simétrica alrededor de su media $ \mu = 0 $, y su f.g.m. tiene la forma:

 $M_X(t) = e^{t^2 / 2}$

  lo cual es una función par: $M_X(-t)= M_X(t)$.

- **¿Qué tan rápido crece la función?**

  Muy rápido. El crecimiento es exponencial cuadrático. Esto implica que los momentos de la normal crecen rápidamente en magnitud.

- **¿Cómo se relaciona esta gráfica con los momentos de la normal?**

  Derivando sucesivamente la f.g.m. en $ t = 0 $, se obtiene:

  $\mathbb{E}[X^k] = M_X^{(k)}(0)$

  Por tanto, la gráfica "encierra" toda la información sobre los momentos.

---

  **🧠 Conclusión**

Estas gráficas te permiten **visualizar la información estadística codificada en una variable aleatoria**. La f.g.m. no es solo una herramienta algebraica para obtener momentos, sino una forma poderosa de describir el comportamiento global de la variable.

> **¿Qué pasa si dos variables tienen la misma f.g.m.?**  
> ¡Tienen la misma distribución! (si la f.g.m. está definida en un entorno de 0).

  **Ejemplo: Distribución uniforme $U(a,b)$**

Si 
$$X \sim U(a,b),$$  
su densidad es  
$$f(x) = \frac{1}{b - a}\quad\text{para }a < x < b,$$  
y su función generadora de momentos viene dada por 

<a id="eq:5.2"></a> 
<table align="center">
  <tr>
    <td align="center">
$$M(t)= \int_a^b \frac{e^{t x}}{b - a}\,dx= \frac{e^{b t} - e^{a t}}{t\,(b - a)}.$$  
    </td>
    <td valign="bottom">
      (5.2)
    </td>
  </tr>
</table>

En el caso particular de la distribución uniforme en $(0,1)$ se obtiene  
$$M(t) = \frac{e^t - 1}{t}.$$

---

Para derivar la fórmula [#(5.2)](#eq:5.2) y obtener los momentos, podemos usar el desarrollo en serie de la función exponencial:

$$M(t)= \frac{1}{t\,(b - a)}\bigl(e^{b t} - e^{a t}\bigr) \\
= \frac{1}{t\,(b - a)}\Bigl[\bigl(1 + \sum_{n=1}^\infty \tfrac{(b t)^n}{n!}\bigr)
                    -\bigl(1 + \sum_{n=1}^\infty \tfrac{(a t)^n}{n!}\bigr)\Bigr] \\
= \frac{1}{b - a}\sum_{n=1}^\infty \frac{b^n - a^n}{n!}\,t^{n-1}.
$$

Este es el desarrollo de Maclaurin de $M(t)$ en $t=0$; por tanto, sus derivadas en cero satisfacen

<table align="center">
  <tr>
    <td align="center">
$$M^{(k)}(0)= \frac{b^{k+1} - a^{k+1}}{(k+1)\,(b - a)}.$$
    </td>
    <td valign="bottom">
      (5.3)
    </td>
  </tr>
</table>
En particular:

-  $$M'(0)= \frac{b^2 - a^2}{2\,(b - a)}= \frac{a + b}{2},$$
  que coincide con $\mathbb{E}(X)$.

-  $$M''(0)= \frac{b^3 - a^3}{3\,(b - a)}= \frac{a^2 + a b + b^2}{3},$$

y un cálculo directo muestra que la varianza es

$$\mathrm{Var}(X)= \mathbb{E}(X^2) - \bigl(\mathbb{E}(X)\bigr)^2= \frac{(a + b)^2}{12}.$$

  **Observación importante** 
Sea $X$ una v.a. con f.g.m. $M_X$ y sea $Y=aX+b$ una transformación lineal de $X$, entonces

$$M_Y(t)=E(e^{tY})=E(e^{t(aX+b)})=E(e^{taX}e^{tb})=e^{tb}E(e^{taX})=e^{tb}M_X(at)$$



  **Teorema (fgm de suma de v.a.s)**

Si $X$ tiene función generadora de momentos $M(t)$ que está definida en un entorno $(-a,a)$ de 0, entonces $M(t)$ caracteriza a la distribución de $X$; es decir, si otra variable $Y$ tiene la misma función generadora de momentos, las distribuciones de $X$ e $Y$ coinciden.

---

Si $X,Y$ son variables aleatorias con funciones generadoras de momentos respectivas $M_X$ y $M_Y$ que existen en un dominio común $|t| < d$, entonces la f.g.m. de la suma $X+Y$ está dada por
<a id="eq:5.5"></a> 
<table align="center">
  <tr>
    <td align="center">
$$M_{X+Y}(t)= \mathbb{E}\bigl[e^{t(X+Y)}\bigr]= \mathbb{E}\bigl[e^{tX}\,e^{tY}\bigr]=\mathbb{E}\bigl[e^{tX}\bigr]\;\mathbb{E}\bigl[e^{tY}\bigr]= M_X(t)\,M_Y(t).
\tag{5.5}
$$
  </td>
    <td valign="bottom">
      (5.5)
    </td>
  </tr>
</table>

Este resultado se extiende a la suma de $n$ variables aleatorias independientes. Si

$$S_n = X_1 + \cdots + X_n,$$

entonces

$$M_{S_n}(t)= \mathbb{E}\bigl[e^{tS_n}\bigr]= \mathbb{E}\Bigl[e^{t\sum_{i=1}^n X_i}\Bigr]= \prod_{i=1}^n\mathbb{E}\bigl[e^{tX_i}\bigr]= \prod_{i=1}^n M_{X_i}(t).$$

La función generadora de momentos resulta particularmente útil cuando consideramos sucesiones de variables aleatorias, como lo muestra el siguiente teorema que enunciamos sin demostración:

---

  **Teorema (de Continuidad)**

Sea $F_n(x)$, $n\ge1$, una sucesión de funciones de distribución con funciones generadoras de momentos respectivas $M_n(t)$, definidas en $|t|<b$. Supongamos que cuando $n\to\infty$,

$$
M_n(t)\,\longrightarrow\,M(t)
\quad\text{para }|t|\le a,
$$

donde $M(t)$ es la función generadora de momentos de la distribución límite $F(x)$. Entonces

$$
F_n(x)\,\longrightarrow\,F(x)
\quad\text{cuando }n\to\infty
$$

para todo punto $x$ en el cual $F$ es continua.

  **Teorema de Laplace–Moivre**

Sea $X_1, X_2, \ldots, X_n$ una sucesión de variables aleatorias **i.i.d.** con distribución \( \text{Bernoulli}(p) \), donde \( 0 < p < 1 \). Sea:

$$
S_n = X_1 + X_2 + \cdots + X_n \sim \text{Binomial}(n, p)
$$

y consideremos la variable tipificada:

$$
Z_n = \frac{S_n - np}{\sqrt{np(1 - p)}}
$$

Entonces, cuando \( n \to \infty \), se tiene convergencia en distribución a una normal estándar:

$$
Z_n \xrightarrow{d} \mathcal{N}(0, 1)
$$

es decir,

$$
\lim_{n \to \infty} \mathbb{P}(Z_n \leq z) = \Phi(z), \quad \text{para todo } z \in \mathbb{R}
$$

donde \( \Phi(z) \) es la función de distribución acumulada de la normal estándar.

---

  **Demostración usando funciones generadoras de momentos**

La función generadora de momentos (mgf) de $S_n \sim \text{Binomial}(n, p)$ es:

$$
M_{S_n}(t) = \left(1 - p + p e^t\right)^n
$$

Queremos obtener la mgf de la variable tipificada \( Z_n \). Usamos la propiedad de cambio de variable de la mgf:

$$
M_{Z_n}(t) = \mathbb{E}\left[ e^{t Z_n} \right]
= \mathbb{E}\left[ e^{t \cdot \frac{S_n - np}{\sqrt{np(1 - p)}}} \right]
= e^{-t \cdot \frac{np}{\sqrt{np(1 - p)}}} \cdot M_{S_n}\left( \frac{t}{\sqrt{np(1 - p)}} \right)
$$

Sustituimos la mgf de \( S_n \):

$$
M_{Z_n}(t) = \exp\left( -t \cdot \frac{np}{\sqrt{np(1 - p)}} \right)
\cdot \left( 1 - p + p e^{t / \sqrt{np(1 - p)}} \right)^n
$$

---

  **Aproximación por series de Taylor**

Expandimos  $e^{t / \sqrt{np(1 - p)}}$ para $n$ grande:

$$
e^{t / \sqrt{np(1 - p)}} = 1 + \frac{t}{\sqrt{np(1 - p)}} + \frac{t^2}{2np(1 - p)} + \cdots
$$

Entonces:

$$
1 - p + p e^{t / \sqrt{np(1 - p)}} \approx 1 + \frac{pt}{\sqrt{np(1 - p)}} + \frac{pt^2}{2np(1 - p)} + \cdots
$$

Usamos que $\log(1 + x) \approx x - \frac{x^2}{2} + \cdots$ para $x \approx 0$:

$$\log M_{Z_n}(t) \approx -t \cdot \frac{np}{\sqrt{np(1 - p)}}+ n \left( \frac{pt}{\sqrt{np(1 - p)}} + \frac{pt^2}{2np(1 - p)} \right)$$

Simplificamos:

- El término lineal se cancela:

$$
-t \cdot \frac{np}{\sqrt{np(1 - p)}} + n \cdot \frac{pt}{\sqrt{np(1 - p)}} = 0
$$

- Queda:

$$
\log M_{Z_n}(t) \to \frac{t^2}{2}, \quad \text{cuando } n \to \infty
$$

Por tanto:

$$
M_{Z_n}(t) \to e^{t^2 / 2}
$$

---

  **Conclusión**

Como $e^{t^2/2}$ es la mgf de $\mathcal{N}(0, 1)$, y por el teorema de unicidad de la función generadora de momentos:

$$
Z_n \xrightarrow{d} \mathcal{N}(0, 1)
$$

Esto concluye la demostración del **Teorema de Laplace–Moivre** utilizando funciones generadoras de momentos.


## Muestra aleatoria simple

Sea $\underset{\sim}{X} =(X_1 ,..., X_n)$ un vector aleatorio. Se dice que sus componentes $X_1 ,..., X_n$ son \textcolor{red}{independientes} si $P(X_1\leq x_1 ,..., X_n\leq x_n)=P(X_1\leq x_1)...P(X_n\leq x_n)$ para cualesquiera valores $x_1,..., x_n$ .
	
Si además la distribución de las $n$ variables aleatorias $X_i$ es la misma, se dice que $X_1 ,...,X_n$ son variables aleatorias **independientes e idénticamente distribuidas**, o bien que son v.a.i.i.d o simplemente i.i.d.
	
Si $\underset{\sim}{X} =(X_1 ,..., X_n)$ y $X_1 ,..., X_n$ son i.i.d. con función de densidad (en su caso, de masa) $f_X$ , la distribución conjunta de $\underset{\sim}{X}$ viene dada por la función de densidad (en su caso, de masa) conjunta
$$
\begin{align*}
f_{\underset{\sim}{X}}(\underset{\sim}{x})&=f_{(X_1 ,..., X_n)}(x_1 ,..., x_n)\\
&=f_{(X_1)}(x_1)...f_{(X_n)}(x_n)\\
&=\prod_{i=1}^{n}f_{(X_i)}(x_i)
\end{align*}
$$

A un vector $\underset{\sim}{X} =(X_1 ,..., X_n)$ de v.a.i.i.d. con distribución igual a la de la variable aleatoria $X$ se le denomina también **muestra aleatoria simple** de $X$ (m.a.s de $X$).

Esto responde al hecho siguiente. Supongamos que se desea estudiar la característica $X$ de los individuos de una población de tamaño infinito. Definimos el experimento consistente en elegir aleatoriamente un individuo de la población y llamamos $X$ al valor de la característica de interés en ese individuo. X es una variable aleatoria.


Si definimos un nuevo experimento consistente en elegir una muestra aleatoria de n individuos y se anota $X_i$, el valor de la característica en el individuo i-ésimo, entonces **X** $=(X_1 ,..., X_n)$ es una colección de n v.a.i.i.d. con distribución igual a la de la variable aleatoria $X$, es decir, $X_1 ,..., X_n$ es una m.a.s. de X.

## Modelo paramétrico
Usualmente la ley de probabilidad de una variable aleatoria se supone perteneciente a un modelo matemático que depende sólo de un número finito de parámetros:
	$f_X \in\{f(x|\theta):\theta \in \Theta \subseteq \mathbb{R}^k\}$.
	Escribiremos alternativamente $f(x;\theta)$, $f(x|\theta)$ o $f_\theta(x)$.

**Definición**
		El conjunto de distribuciones dadas por $f_\theta(x)$, $\theta \in \Theta$ se llama familia paramétrica de distribuciones. $\Theta$ es el conjunto de parámetros.
	

**Definición**
	La correspondiente distribución conjunta de una muestra aleatoria simple de $X$ viene dada por la función de densidad (o función de masa de probabilidad, según el caso)

$$
f_{\underset{\sim}{X}}(\underset{\sim}{x} \mid \theta) = \prod_{i=1}^{n} f_{\theta}(x_i)
$$

A esta función la llamaremos **función de verosimilitud** de la muestra $X_{\sim}$. Utilizaremos este término para referirnos indistintamente a la función de densidad conjunta (si las variables aleatorias son continuas) o a la función de masa conjunta (si son discretas).
	

## Sumas de variables aleatorias
Cuando se obtiene una muestra aleatoria simple $X_{1},X_{2},\ldots,X_{n}$ normalmente se calculan a partir de ellas cantidades que resumen los valores observados. Cualquiera de estos resúmenes se puede expresar como una función $T(x_1,\ldots,x_n)$ definida en el espacio $\mathcal{X}^n\subseteq\mathbb{R}^n$ donde están las imágenes del vector $(X_{1},X_{2},\ldots,X_{n})$.

Esta función $T$ puede devolver valores de $\mathbb{R}$, $\mathbb{R}^2$ o, en general, $\mathbb{R}^k$.

$$T(X_1 , \ldots, X_n)=\sum_{i=1}^{n}X_i,\bar{X},\bar{X}+3, \min{X_1 , \ldots, X_n},$$ 
$$T(X_1 , \ldots, X_n)=\left(\sum_{i=1}^{n}X_i,\sum_{i=1}^{n}(X_i-\bar{X})^2\right),$$
$$T(X_1 , \ldots, X_n)=\left(\min\{X_1 , \ldots, X_n\},\sum_{i=1}^{n}X_i,\sum_{i=1}^{n}(X_i-\bar{X})^2\right),$$
$$T(X_1 , \ldots, X_n)= (X_1 , \ldots, X_n)$$


**Definición de estadísticos:** Las funciones $T$ que dependen de una muestra aleatoria simple $X_1 , \ldots, X_n$ se llaman **estadísticos**. Dependen de los valores observados, pero no de los
	parámetros desconocidos que determinan la distribución de $X_i$ . 	

Cuando un estadístico $T$ es utilizado con el propósito de estimar un parámetro $\theta$ diremos que $T$ es un estimador de $\theta$.	

**Ejemplo de estadístico**

$T(X_1 , \ldots, X_n)=\bar{X}$ es un estimador de $E(X)=\mu$.

En inferencia estadística interesa saber qué estadísticos son suficientes para recoger toda la información que la muestra aporta sobre la distribución de la variable aleatoria X muestreada. La respuesta depende de la distribución de X.

**Definición distribución en el muestreo:** Dado que $\underset{\sim}{X} =(X_1 ,..., X_n)$ es una variable aleatoria, se tiene que $Y=T(\underset{\sim}{X})=T(X_1 ,..., X_n)$ será también una variable aleatoria. La ley de probabilidad de $Y$ se denomina **distribución en el muestreo de $Y$** (o distribución muestral). Los siguientes resultados dan información sobre algunas características de estadísticos definidos a partir de sumas de variables aleatorias.

## Estadísticos definidos a partir de sumas de variables aleatorias

**Teorema** Sean $X_1,\ldots, X_n$,n números reales, sea $\bar{x} = \frac{1}{n} \sum_{i=1}^{n}x_i$ su media aritmética y sea $S_n^2=\frac{\sum_{i=1}^{n}(x_i-\bar{x})^2}{n-1}$ su varianza muestral.

- $\min_a\sum_{i=1}^{n}(x_i-a)^2=\sum_{i=1}^{n}(x_i-\bar{x})^2$
- $(n-1)S_n^2=\sum_{i=1}^{n}(x_i-\bar{x})^2=\sum_{i=1}^{n}x_i^2-n\bar{x}^2$

**Lema** Sea $X_1,\ldots, X_n$ una muestra aleatoria simple de $X$ y sea $g(x)$ una función tal que $E(g(X))$ y $Var(g(X))$ existen. Entonces,

- $E(\sum_{i=1}^{n}g(X_i))=nE(g(X))$,
- $Var(\sum_{i=1}^{n}g(X_i))=nVar(g(X))$.

Para la demostración ver Gómez et al. (2006)

**Teorema** Sea $X 1,\ldots, X_n$ una muestra aleatoria simple de una población $X$ con esperanza $\mu$ y varianza $\sigma^2 < \infty$. Sean
$$
		\begin{align*}
		&\bar{X}=\frac{1}{n}\sum_{i=1}^{n}X_i,\ \
		S^2=\frac{\sum_{i=1}^{n}(X_i-\bar{X})^2}{n-1},
		\end{align*}	
$$
la media y la varianza muestrales, respectivamente. Entonces,

a. $E(\bar{X}) = \mu,$
b. $Var(\bar{X}) = \frac{\sigma^2}{n},$
c. $E(S^2) = \sigma^2$.


**Teorema** Sea $X 1,\ldots, X_n$ una muestra aleatoria simple de una población $X$ con función generadora de momentos $M_X(t)$. La función generatriz de momentos de $X$ es
$$\begin{align*}
		&M_{\bar{X}}(t)=\left(M_X\left(\frac{t}{n}\right)\right)^n
		\end{align*}
$$
::: {.theorem #teo6-3}
**Teorema (Combinación lineal de normales es normal)**  (Wackerly et al. (2008))
Sean $Y_1,\,Y_2,\cdots,\,Y_n$  variables aleatorias independientes normalmente distribuidas $E(Y_i)=\mu_i$ y $V(Y_i)=\sigma_i^2$ara $i=1,\cdots,\,n$ y sean $a_1,\,a_2,\cdots,\,a_n$ constantes. Si $$U=\sum_{i=1}^na_iY_i$$

entonces $U$ es una variable aleatoria normalmente distribuida con
	$$E(U)=\sum_{i=1}^na_i\mu_i$$
	y 
	$$V(U)=\sum_{i=1}^na_i^2\sigma^2_i$$
:::

**Ejemplo** $X 1,\ldots, X_n$ m.a.s. de $X \sim N(\mu,\sigma^2)$. Entonces, $M_{X}(t)=\exp\left\{\mu t+ \frac{\sigma^2t^2}{2}\right\}$. De ahí que 

$$
	\begin{align*}
	M_{\bar{X}}(t)
	&=\left(\exp\left\{\mu \frac{t}{n}+ \frac{\sigma^2\left(\frac{t}{n}\right)^2}{2}\right\}\right)^n
	\end{align*}
$$


$X 1,\ldots, X_n$ m.a.s. de $X \sim N(\mu,\sigma^2)$. Entonces, $M_{X}(t)=\exp\left\{\mu t+ \frac{\sigma^2t^2}{2}\right\}$. De ahí que 
$$
		\begin{align*}
		M_{\bar{X}}(t)&=\exp\left\{\mu t+ \frac{\sigma^2t^2}{2n}\right\}
		\end{align*}
$$
De ahí que $\bar{X}\sim N(\mu,\frac{\sigma^2}{n})$.

## Muestreo de una distribución normal

### Definición de distribución Chi cuadrada

::: {.theorem #def-4.10}
**Definición** (Wackerly et al. (2008)) 
Sea $\nu$ un entero positivo. Se dice que una v.a $Y$ tiene distribuci\'on **chi cuadrada con $\nu$ grados de libertad** si y sólo si $Y$ es una vriable aleatoria con distribución gamma y parámetros $\alpha=\nu/2$ y $\beta=2$.
:::

::: {.theorem #teo-Fisher}
**Teorema de Fisher** En el resto del tema supondremos que $X 1,\ldots, X_n$ m.a.s. de una $N(\mu, \sigma^2)$.

a. $\bar{X}$ y $S_n^2$ son variables aleatorias independientes.
b. $\bar{X}\sim N(\mu, \frac{\sigma^2}{n})$
c. $\frac{(n-1)S_n^2}{\sigma^2}\sim \mathcal{X}^2_{n-1}.$
:::

### Distribuciones asociadas a la normal

::: {.theorem #teo6-4}
**Teorema** (Wackerly et al. (2008)) Sean $Y_1,\,Y_2,\cdots,\,Y_n$ definidas como en el Teorema 6.3 de \cite{Wackerly} y definimos $Z_i$ por 
	$$Z_i=\frac{Y_i-\mu_i}{\sigma_i}$$
	con $i=1,\,2,\cdots,\,n$. Entonces $\sum_{i=1}^nZ_i^2$ tiene distribuici\'on $\chi^2$ con $n$ grados de libertad. 
:::

::: {.theorem #teo7-2}
**Teorema ** (Wackerly et al. (2008)) Si $Y_1,\,Y_2,\cdots,\,Y_n$ es una muestra aleatoria de una distribuci\'on normal con media $\mu$ y varianza $\sigma^2$, $Y_i$, $i=1,\,2,\cdots,n$ son v.a's independientes distribu\'idas normalmente, con $E(Y_i)=\mu$ y $V(Y_i)=\sigma^2$.

Entonces $$Z_i=\frac{Y_i-\mu}{\sigma}$$
		son v.a's independientes, $i=1,\,2,\cdots,n$  y 
		$$\sum_{i=1}^nZ_i^2=\sum_{i=1}^n\left(\frac{Y_i-\mu}{\sigma}\right)^2$$tienen una distribuci\'on $\chi^2$ con $n$ grados de libertad (gl).
:::

::: {.theorem #teo7-3}
**Teorema** (Wackerly et al. (2008)) Sea $Y_1,\,Y_2,\cdots,\,Y_n$ una muestra aleatoria con media $\mu$ y varianza $\sigma^2$. Entonces
		$$\frac{(n-1)S^2}{\sigma^2}=\frac{1}{\sigma^2}\sum_{i=1}^n(Y_i-\overline{Y})^2$$
		tiene una distribuci\'on $\chi^2$ con $(n-1)$ gl. $\overline{Y}$ y $S^2$ son v.a independientes.
:::

::: {.theorem #def7-2}
**Definición** (Wackerly et al. (2008))
Sea $Z$ una v.a normal est\'andar y sea $W$ una v.a con distribuci\'on $\chi^2_\nu$. Entonces, si $W$ y $Z$ son ind
		$$T=\frac{Z}{\sqrt{W/\nu}}$$
		se dice que tiene una distribuci\'on $t$ con $\nu$ grados de libertad.
:::



**Observación** Si $Y_1,\,Y_2,\cdots,\,Y_n\sim N(\mu,\sigma^2)$ del Teorema \@ref(teo7-1)
$$Z=\frac{\sqrt{n}(\overline{Y}-\mu)}{\sigma}\sim N(0,1)$$
El teorema \@ref(teo7-3) nos dice que
$$W=\frac{(n-1)S^2}{\sigma^2}\sim\chi^2_{n-1}$$
y que $Z$ y $W$ son ind.





\begin{frame}{1.2.1. Distribuciones asociadas a la normal}
\begin{remark}
Por tanto, de la definici\'on 7.2
\begin{eqnarray*}
	T&=&\frac{Z}{\sqrt{W/\nu}}\\
	&=&\frac{\sqrt{n}(\overline{Y}-\mu)/\sigma}{\sqrt{\left[\frac{(n-1)S^2}{\sigma^2}\right]/(n-1)}}\\
	&=&\sqrt{n}\left(\frac{\overline{Y}-\mu}{S}\right)
\end{eqnarray*}
tiene distribuci\'on $t$ con $(n-1)$ grados de libertad.
\end{remark}
\end{frame}
\begin{frame}{1.2.1. Distribuciones asociadas a la normal}
\begin{defi}[Definici\'on 7.3]{\em Sean $W_1$ y $W_2$ v.a's independientes con distribuci\'on $\chi^2$, con $\nu_1$ y $\nu_2$  grados de libertad respectivamente. Entonces se dice que:
		$$F=\frac{W_1/\nu_1}{W_2/\nu_2}$$
		tiene una distribuc\'on $F$ con $\nu_1$ grados de libertad en el numerador y $\nu_2$ grados de libertad en el denominador.}
	\end{defi} 
\end{frame}

\begin{frame}{1.2.1. Distribuciones asociadas a la normal}
	\begin{remark}
	Considerando dos muestras aleatorias independientes tomadas de distribuiciones normales
	$$W_1=\frac{(n_1-1)S_1^2}{\sigma_1^2}\sim\chi^2_{n_1-1}$$
	$$W_1=\frac{(n_2-1)S_2^2}{\sigma_2^2}\sim\chi^2_{n_2-1}$$
	$W_1\bot W_2$.
	\end{remark}
\end{frame}

\begin{frame}{1.2.1. Distribuciones asociadas a la normal}
	\begin{remark}
		\begin{eqnarray*}
			F&=&\frac{W_1/\nu_1}{W_2/\nu_2}\\
			&=&\frac{[(n_1-1)S_1^2/\sigma_1^2]/(n_1-1)}{[(n_2-1)S_2^2/\sigma_2^2]/(n_2-1)}\\
			&=&\frac{S_1^2/\sigma_1^2}{S_2^2/\sigma_2^2}
		\end{eqnarray*}
		tiene distribuci\'on $F$ con $(n_1-1)$ gl en el numerador y $(n_2-1)$ gl en el denominador
	\end{remark}
\end{frame}

\begin{frame}
	\begin{center}
		\includegraphics[width=0.8\textwidth]{fig74}
	\end{center}
\end{frame}

\begin{frame}
	\begin{center}
		\includegraphics[width=0.8\textwidth]{T7}
	\end{center}
\end{frame}

\begin{frame}
	\begin{ejemplo}[Ejemplo 7.7]{\em $$Y_1^1,\,Y_2^1\,\cdots,\,Y_{n_1}^1\sim N(\mu_1,\,\sigma^2)$$
			$$Y_1^2,\,Y_2^2\,\cdots,\,Y_{n_2}^2\sim N(\mu_2,\,\sigma^2)$$
			$P\left(\frac{S_1^2}{S_2^2}\leq b\right)=0.95$ con $n_1=6$ y $n_2=10$, ?`$b$?}
	\end{ejemplo}
	Como $n_1=6$ y $n_2=10$ y las varianzas poblacionales son iguales, entonces 
	$\frac{S_1^2/\sigma_1^2}{S_2^2/\sigma_2^2}=\frac{S_1^2}{S_2^2}\sim F_{5,9}$
	$$P\left(\frac{S_1^2}{S_2^2}\leq b\right)= F_{5,9}(b)=0.95$$
	entonces $qf(0.95,\,5,\,9)=b$, $b=3.48$.
\end{frame}

\begin{frame}
\begin{ejemplo}Experimento en R, generar n\'umeros aleatorios exponenciales (1/10). Por ser los datos distribuídos exponecialmente, su media será $\mu=10$ y la {\tiny$\sigma^2=100$.
		$$\begin{array}{ccccc}\mbox{Tama\~no muestral}&\mbox{Promedio muestreo repetido}&\mbox{Media teórica} \mu&\mbox{Varianza muestreo repetido}&\mbox{Varianza teórica}\frac{\sigma^2}{n}\\n=5&10.07389&10&19.01144&20\\n=25&10.04106&10&3.6275184&4\end{array}$$}
\end{ejemplo}
\end{frame}

\begin{frame}
Resultado previo a la prueba del Teorema 7.4 sin prueba
\begin{teo}[Teorema 7.5]Sean $Y_1,\,Y_2,\cdots,\,Y_n$  v.a's  con funciones generadoras de momentos $m(t)$  y $m_1(t),\,m_2(t),\cdots,$ respectivamente. Si 
	$$\lim_{n\rightarrow\infty}m_n(t)=m(t)\mbox{ para toda $t$ real,}$$
	entonces la funci\'on de distribuci\'on de $Y_n$ converge hacia la funci\'on de distribuci\'on de $Y$ cuando $n\rightarrow\infty$
\end{teo}
\end{frame}

\section{1.3. Leyes de los Grandes Números y Teorema Central del Límite }

\subsection{1.3.1. Leyes de los grandes números}
\begin{defi}
Una sucesión de variables aleatorias converge en media a $X$, y se denota por $X_{n}\xrightarrow{cm}X$ , si para cualquier $\epsilon>0$ se tiene que:  
\[\lim _{n\to \infty }E(\left|X_{n}-X\right|)=0,\]
siempre que dicha esperanza exista.\\

De forma análoga se define convergencia en media de orden r si:
\[\lim _{n\to \infty }E(\left|X_{n}-X\right|^r)=0,\]

Cuando $r=2$ se dice que se tiene convergencia en media cuadrática

\[\lim _{n\to \infty }E(\left|X_{n}-X\right|^2)=0,\]

\end{defi}

\begin{frame}{Relaciones entre tipos de convergencias}
	\begin{center}
		\includegraphics[width=0.7\textwidth]{esquema}
	\end{center}
\end{frame}

\begin{frame}{Ley débil de los grandes números}
\begin{teo}
Sea $X 1,\ldots, X_n$ una sucesión de variables aleatorias incorreladas con momentos de segundo orden acotados por una constante $C$, independiente de $n$. Sea $S_n = \sum_{i=1  }^{n}X_i$. Entonces

\begin{align}
E\left(\left|\frac{S_n-E(S_n)}{n}\right|^2\right)\leq \frac{C}{n}
\end{align}
\end{teo}
\end{frame}


\begin{frame}{Ley débil de los grandes números}

		
		y, como consecuencia
		
		\[\lim _{n\to \infty }\frac{S_n-E(S_n)}{n}=0,\]
		
		en el sentido de la convergencia en \textcolor{red}{media cuadrática}.

\end{frame}

\begin{frame}
Los resultados que garantizan la convergencia casi segura de la media muestral se conocen como leyes fuertes de los grandes números. Se enuncia a continuación una \textcolor{red}{ley fuerte} para variables con segundos momentos finitos e
incorreladas.
\end{frame}

\begin{frame}{Ley fuerte de los grandes números}
	\begin{teo}
		Sea $X 1,\ldots, X_n$ una sucesión de variables aleatorias incorreladas con momentos de segundo orden acotados por una constante $C$, independiente de $n$. Sea $S_n = \sum_{i=1  }^{n}X_i$. Entonces
		
		\begin{align}
		E\left(\left|\frac{S_n-E(S_n)}{n}\right|^2\right)\leq \frac{C}{n}
		\end{align}
	\end{teo}
\end{frame}


\begin{frame}{Ley fuerte de los grandes números}
	
	
	y, como consecuencia
	
	\[\lim _{n\to \infty }\frac{S_n-E(S_n)}{n}=0,\]
	
	en el sentido de la \textcolor{red}{casi segura}.
	
\end{frame}



\subsection{1.3.2. Teorema central del límite}
\begin{frame}{1.3.2. Teorema central del límite}
\begin{teo}[Teorema 7.4, Teorema del l\'imite central \cite{Wackerly}] Sean $Y_1,\,Y_2,\cdots,\,Y_n$  v.a's iid ( no se precisa de que distribuci\'on se generan) con $E[Y_i]=\mu$ y $V[Y_i]=\sigma^2<\infty$. Definamos 
	$$U_n=\frac{\sum_{i=1}^nY_i-n\mu}{\sigma\sqrt{n}}=\frac{\overline{Y}-\mu}{\sigma/\sqrt{n}}\mbox{ donde} \overline{Y}=\frac{1}{n}\sum_{i=1}^nY_i$$
	\end{teo}
\end{frame}

\begin{frame}{1.3.2. Teorema central del límite}
	
		Entonces la funci\'on de distribuci\'on de $U_n$ converge hacia la funci\'on de distribuci\'on normal est\'andar cuando n tiende a infinito . Esto es,
		$$\lim_{n\rightarrow\infty}P(U_n\leq u)=\int_{-\infty}^u\frac{1}{\sqrt{2\pi}}e^{-t^2/2}dt\,\forall u$$
\end{frame}

---
## Referencias

- **Gómez, Guadalupe**, & **Delicado, Pedro** (2006). *Curso de Inferencia y Decisión*. Departament d’Estadística i Investigació Operativa, Universitat Politècnica de Catalunya.

- **Wackerly, D. D., Mendenhall, W.**, & **Scheaffer, R. L.** (2008). **Estadística matemática con aplicaciones** (7ª ed.). Cengage Learning.

- **Roussas, G. G.** (1997). **A Course in Mathematical Statistics** (2nd ed.). Academic Press.
