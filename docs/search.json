[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Inferencia Estad√≠stica",
    "section": "",
    "text": "1 Introducci√≥n\nEste libro ha sido concebido como un recurso integral para el estudio riguroso y aplicado de la inferencia estad√≠stica. Est√° dirigido a estudiantes de programas de estad√≠stica, matem√°ticas aplicadas y disciplinas afines, y busca fortalecer la comprensi√≥n conceptual y t√©cnica de los fundamentos que sustentan el an√°lisis estad√≠stico moderno.\nA lo largo de sus cap√≠tulos, el lector encontrar√° un desarrollo progresivo de los siguientes temas:\nEl material combina el rigor formal con ejemplos y aplicaciones que ilustran c√≥mo los m√©todos estad√≠sticos permiten extraer conclusiones v√°lidas a partir de datos.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Inferencia Estad√≠stica</span>"
    ]
  },
  {
    "objectID": "Datos y modelos.html",
    "href": "Datos y modelos.html",
    "title": "2¬† Datos y modelos",
    "section": "",
    "text": "3 DATOS Y MODELOS\nEn esta secci√≥n establecemos la base conceptual para el an√°lisis estad√≠stico, diferenciando claramente entre el fen√≥meno aleatorio observado y la medida de probabilidad que lo describe.\n\n3.0.0.1 1.1.1 Fen√≥meno aleatorio y variable observada\n\n‚ÄúSe observa una realizaci√≥n de un fen√≥meno aleatorio, digamos X. Este puede ser un elemento aleatorio de varios tipos: n√∫mero (variable aleatoria), un vector de dimensi√≥n finita (vector aleatorio), una funci√≥n, etc.\nLa premisa principal es que el car√°cter aleatorio de X se concibe como una realizaci√≥n de un fen√≥meno aleatorio que tiene una distribuci√≥n de probabilidad P, donde la distribuci√≥n P es desconocida ya sea en su totalidad o en alg√∫n detalle espec√≠fico (por ejemplo, su soporte, su media, etc.). Es de inter√©s conocer P. Si la medida de probabilidad P fuese conocida, entonces no hay problema estad√≠stico propiamente, pues el problema estad√≠stico tiene que ver con inferir la propiedad desconocida de P con base en X.‚Äù [Ver referencia 1]\n\n\nDefinici√≥n de X\n\nX puede ser un valor real \\(X \\in \\mathbb{R}\\), un vector en \\(\\mathbb{R}^n\\), o incluso una funci√≥n \\(\\;X: [0,1]\\to\\mathbb{R}\\).\n\n\nMedida de probabilidad P\n\nDesconocida: soporte, media, varianza, etc.\n\nObjetivo estad√≠stico: inferir caracter√≠sticas de (P) a partir de la muestra (la realizaci√≥n de X).\n\n\n\n\n3.0.0.2 1.1.2 Incertidumbre inductiva vs.¬†estoc√°stica\n\n‚ÄúLa observaci√≥n X est√° dada, por lo que no hay incertidumbre tal como la hay en la teor√≠a de probabilidad desarrollada anteriormente en el curso. Antes, fue concebida una estructura \\((\\Omega, \\mathcal{F}, P)\\) para enfrentar el que haya incertidumbre acerca del valor de X. En el problema estad√≠stico, el valor de X ha sido observado, y la incertidumbre radica en otro punto: radica en que existe duda acerca de cu√°l P es la que produjo el valor X. En algunas ocasiones se utilizan los t√©rminos incertidumbre estoc√°stica e incertidumbre inductiva para distinguir estos dos tipos. Es com√∫n que estos se confundan entre s√≠, porque en estad√≠stica matem√°tica la teor√≠a de probabilidad constituye tambi√©n una de las maneras naturales de afrontar la cuantificaci√≥n de incertidumbre inductiva. En cualquier caso, el concebir a P como medida de probabilidad es la base para formular soluciones a la incertidumbre inductiva. Con este lenguaje, probabilidad y estad√≠stica son problemas diferentes y de cierta manera inversos. Teor√≠a de probabilidad tiene que ver con cuantificar incertidumbre acerca de X y teor√≠a estad√≠stica con cuantificar incertidumbre acerca de P a la luz de haber ya observado X.‚Äù[Ver referencia 1]\n\n\nIncertidumbre estoc√°stica: duda previa sobre el valor de X, modelada por \\((\\Omega,\\mathcal{F},P)\\).\n\nIncertidumbre inductiva: tras observar X, la incertidumbre se desplaza a la ley generadora P.\n\n\n\n\n3.0.0.3 1.1.3 Ejemplos en Matem√°tica Aplicada e Ingenier√≠a de Sistemas\n\nModelado de tiempos de respuesta en redes\n\n\\(X\\): tiempo de llegada de paquetes (variable continua).\n\n\\(P\\): distribuci√≥n de retardo desconocida; objetivo: estimar par√°metros de una ley de colas M/M/1.\n\nEstimaci√≥n de par√°metros en ecuaciones diferenciales estoc√°sticas\n\n\\(X(t)\\): trayectoria observada de un proceso de It√¥.\n\n\\(P\\): ley del proceso (por ejemplo, coeficientes de difusi√≥n y deriva), inferidos a partir de trayectorias discretas.\n\nCalibraci√≥n de sensores en sistemas de control\n\n\\(X\\): lecturas del sensor (vector aleatorio).\n\n\\(P\\): distribuci√≥n conjunta desconocida de ruido; se estima para dise√±ar filtros de Kalman √≥ptimos.\n\n\n\nCon esta distinci√≥n clara entre dato observado y modelo probabil√≠stico, estamos listos para construir estimadores y desarrollar la inferencia estad√≠stica en las secciones siguientes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Datos y modelos</span>"
    ]
  },
  {
    "objectID": "Variable aleatoria.html",
    "href": "Variable aleatoria.html",
    "title": "3¬† Variable aleatoria",
    "section": "",
    "text": "4 1.2 VARIABLE ALEATORIA\n\n4.0.0.1 1.2.1 Variables y vectores aleatorios\nConsideramos un experimento aleatorio cuyos resultados pertenecen al espacio muestral Œ©. Modelamos este proceso suponiendo que existe una terna \\((\\Omega, \\mathcal{A}, P),\\) donde:\n\n\\(\\Omega\\) es el espacio muestra,\n\n\\(\\mathcal{P}(\\Omega)\\) es el conjunto de partes de Œ©,\n\n\\(\\mathcal{A}\\in\\mathcal{P}(\\Omega)\\) es una œÉ-√°lgebra,\n\n\\(P\\colon \\mathcal{A} \\to [0,1]\\) es una medida de probabilidad que refleja las caracter√≠sticas aleatorias del experimento realizado.\n\nA esta terna se le llama espacio de probabilidad.\nLos resultados de un experimento aleatorio no son analizados ‚Äúen bruto‚Äù, sino que se les da una representaci√≥n num√©rica que facilita su tratamiento. Esto se logra introduciendo variables aleatorias, que asocian cada resultado \\(\\omega\\in \\Omega\\) con un valor num√©rico o vectorial, y sobre las cuales luego aplicamos t√©cnicas de inferencia estad√≠stica.\nEn todo estudio estad√≠stico partimos de un experimento aleatorio cuyo conjunto de resultados posibles se denomina espacio muestral Œ©. Para cuantificar dichos resultados definimos las siguientes estructuras:\nDefinici√≥n: Variables Aleatorias Sea \\((\\Omega,\\mathcal{A},P)\\) un espacio de probabilidad. Una variable aleatoria es una funci√≥n \\(X\\colon (\\Omega,\\mathcal{A})\\;\\longrightarrow\\; (\\mathbb{R},\\mathcal{B}),\\) tal que para todo \\(B\\in\\mathcal{B}\\) (la \\(\\sigma\\)-√°lgebra de Borel en ‚Ñù), \\(X^{-1}(B)\\;=\\;\\{\\omega\\in\\Omega : X(\\omega)\\in B\\}\\;\\in\\;\\mathcal{A}.\\)\n\nSi el espacio muestral Œ© es finito o numerable, diremos que es un espacio discreto y las variables aleatorias asociadas al experimento normalmente estar√°n definidas como \\(X\\colon \\Omega \\;\\longrightarrow\\; \\mathbb{Z}.\\)\nSi \\(\\Omega\\) es no numerable, entonces diremos que es un espacio continuo y \\(X\\colon \\Omega \\;\\longrightarrow\\; \\mathbb{R}.\\)\n\n\nDefinici√≥n: Vector aleatorio\nUn vector aleatorio de dimensi√≥n \\(n\\) es \\(\\mathbf{X} = (X_1,\\dots,X_n)\\colon(\\Omega,\\mathcal{A})\\longrightarrow(\\mathbb{R}^n,\\mathcal{B}^n),\\) donde cada componente \\(X_i\\) es variable aleatoria y \\(\\mathcal{B}^n\\) la \\(\\sigma\\)-√°lgebra de Borel en ‚Ñù‚Åø.\n\nEjemplos Lanzamiento de dos monedas\nSea \\(\\Omega =\\{\\,CC,\\;C-,\\;-C,\\;--\\},\\) donde \\(C\\) = ‚Äúcara‚Äù y \\(-\\) = ‚Äúcruz‚Äù. Podemos definir:\n\\(X_1(\\omega) = \\text{n√∫mero de caras en }\\omega.\\) \\(X_2(\\omega) = 2 - X_1(\\omega)\\;=\\; \\text{n√∫mero de cruces}.\\) \\(X_3(\\omega) = \\bigl(X_1(\\omega)\\bigr)^2.\\)\nEntonces \\((X_1,X_2,X_3)\\) es un vector aleatorio de dimensi√≥n 3.\nTiempos de servicio en un servidor\nSean \\(T_i\\) los tiempos de servicio (en segundos) de las peticiones \\(i=1,2,3\\). Definimos\n\\(\\mathbf{T}=(T_1,T_2,T_3),\\quad S = T_1 + T_2 + T_3,\\quad M = \\max\\{T_1,T_2,T_3\\}.\\)\nLecturas de sensores en red distribuida\nEn tres nodos \\(i=1,2,3\\) medimos temperatura \\(X_{i,1}\\), presi√≥n \\(X_{i,2}\\) y humedad \\(X_{i,3}\\). El vector global es \\(\\mathbf{X} = (X_{1,1},X_{1,2},X_{1,3},\\,X_{2,1},\\dots,X_{3,3}) \\in \\mathbb{R}^9.\\)\n\nCon estas definiciones rigurosas disponemos ya de los objetos b√°sicos para, en las siguientes secciones, construir estimadores, estudiar su comportamiento asint√≥tico y contrastar hip√≥tesis sobre la distribuci√≥n subyacente \\(P\\).\n\n\n4.0.0.2 1.2.2 Distribuci√≥n de una variable aleatoria. Funciones de distribuci√≥n, de probabilidad y de densidad\nDistribuci√≥n de una Variable Aleatoria\nLa realizaci√≥n de un experimento aleatorio da lugar a un resultado \\(\\omega\\in\\Omega\\) que es aleatorio. Por lo tanto, \\(X(\\omega)\\) es un valor de \\(\\mathbb{R}\\) tambi√©n aleatorio. Es decir, la variable aleatoria \\(X\\) induce una medida de probabilidad en \\(\\mathbb{R}\\). A esa medida de probabilidad se le llama distribuci√≥n de \\(X\\) o ley de \\(X\\). Una de las formas de caracterizar la distribuci√≥n de una variable aleatoria es dar su funci√≥n de distribuci√≥n \\(F_X\\), que est√° definida as√≠:\n\\(F_X(x) \\;=\\; P(X \\le x)\\;=\\; P\\bigl(\\{\\omega \\in \\Omega : X(\\omega) \\le x\\}\\bigr)\\;=\\; P\\bigl(X^{-1}((-\\infty, x])\\bigr).\\)$\nEn el caso de que \\(X\\) sea una variable aleatoria discreta, es decir, en el caso de que \\(X\\) solo tome una cantidad finita o numerable de valores de \\(\\mathbb{R}\\), su distribuci√≥n tambi√©n puede caracterizarse por su funci√≥n de probabilidad (o funci√≥n de masa de probabilidad) \\(f_X\\), definida como\n\\[f_X : \\mathbb{R} \\longrightarrow [0,1],\\qquad f_X(x) = P(X = x).\\]\nEsa funci√≥n solo es no nula en un conjunto finito o numerable. Supondremos en adelante, sin p√©rdida de generalidad, que ese conjunto est√° contenido en \\(\\mathbb{Z}\\). A partir de la funci√≥n de masa de probabilidad se puede calcular la probabilidad de que la variable aleatoria \\(X\\) tome valores en cualquier elemento \\(A \\subseteq \\mathbb{B}\\):\n\\(P(X \\in A) = \\sum_{x \\in A} f_X(x).\\) me\nLa funci√≥n de distribuci√≥n y la funci√≥n de masa de probabilidad se relacionan de la siguiente forma:\n\\(F_X(x) = \\sum_{u \\leq x} f_X(u), \\quad f_X(x) = F_X(x) - F_X(x^-),\\) donde \\(F_X(x^-) = \\lim_{h \\to 0^+} F_X(x - h)\\).\nUna clase relevante de variables aleatorias no discretas son las que poseen funci√≥n de densidad, es decir, aquellas cuya distribuci√≥n de probabilidad puede caracterizarse por una funci√≥n \\(f_X(x) \\geq 0\\) que cumple que:\n\\(P(X \\in A) = \\int_{x \\in A} f_X(x) \\, dx, \\quad \\text{para todo } A \\subseteq \\mathbb{B}.\\)\nLa relaci√≥n entre \\(F_X\\) y \\(f_X\\) es la siguiente:\n\\(F_X(x) = \\int_{-\\infty}^{x} f_X(u) \\, du, \\quad f_X(x) = \\frac{d}{dx} F_X(x),\\)\nsalvo quiz√°s en un n√∫mero finito de puntos \\(x \\in \\mathbb{R}\\). Las variables aleatorias que poseen funci√≥n de densidad se llaman variables aleatorias absolutamente continuas. Abusando del lenguaje, aqu√≠ nos referiremos a ellas como variables aleatorias continuas.\n\n\n4.0.0.3 1.2.3 Esperanza y varianza\nSi se desea describir totalmente la distribuci√≥n de probabilidad de una variable aleatoria \\(X\\) acabamos de ver que podemos dar su funci√≥n de distribuci√≥n o su funci√≥n de masa o de densidad, seg√∫n el caso. Una descripci√≥n parcial puede efectuarse calculando algunas caracter√≠sticas de la variable aleatoria \\(X\\), como por ejemplo medidas de posici√≥n o de dispersi√≥n. Estudiaremos algunas de ellas.\nSe define la esperanza de una variable aleatoria \\(X\\) como la integral de Lebesgue de \\(X\\):\n\\(E(X) = \\int_{\\Omega} X(w) dP(w).\\)\nEn el caso de variables aleatorias discretas la esperanza puede calcularse como:\n\\(E(X) = \\sum_{w \\in \\Omega} X(w) P(w) = \\sum_{k \\in \\mathbb{Z}} k P(X = k) = \\sum_{k \\in \\mathbb{Z}} k f_X(k).\\)\nPor otro lado, la esperanza de una variable aleatoria continua se puede calcular as√≠:\n\\(E(X) = \\int_{\\mathbb{R}} x f_X(x) dx.\\)\nLa esperanza de una variable aleatoria \\(X\\) es una medida de posici√≥n de \\(X\\): es el centro de gravedad de la distribuci√≥n de probabilidad de \\(X\\).\nSi \\(h\\) es una funci√≥n medible \\(h : \\mathbb{R} \\rightarrow \\mathbb{R}\\), entonces \\(Y = h(X)\\) es tambi√©n variable aleatoria y su esperanza se puede calcular a partir de la distribuci√≥n de \\(X\\):\n\\(E(h(X)) = \\int_{\\Omega} h(X(w)) dP(w)\\) que en el caso de que \\(X\\) sea discreta puede reescribirse como\n\\(E(h(X)) = \\sum_{k \\in \\mathbb{Z}} h(k) f_X(k).\\)\nSi \\(X\\) es una variable aleatoria continua entonces\n\\(E(h(X)) = \\int_{\\mathbb{R}} h(x) f_X(x) dx.\\)\nSi existe \\(\\mu = E(X)\\) y es finita puede definirse una medida de dispersi√≥n de la variable aleatoria \\(X\\) a partir de una transformaci√≥n \\(h\\) de \\(X\\). Es lo que se denomina varianza de \\(X\\) y se define as√≠:\n\\(V(X) = E((X - \\mu)^2) = E(X^2) - \\mu^2 = E(X^2) - (E(X))^2.\\)\n\n\n4.0.0.4 1.2.4 Funci√≥n generadora de momentos\nDada una variable aleatoria \\(X\\), o su funci√≥n de distribuci√≥n \\(F\\), vamos a definir otra funci√≥n generadora, como\n\\(M_X(t) = \\mathbb{E}(e^{tX}),\\) siempre que este valor esperado exista.\nNotemos que cuando \\(X\\) toma valores en los enteros no-negativos, \\(M_X(t) = \\phi_X(e^t)\\), donde \\(\\phi_X(s)=E[s^X]=\\sum_{k=0}^{\\infty}p_ks^k\\) para \\(s\\in[0,1]\\) es la funci√≥n generadora de probabilidad (f.g.p.) de la variable \\(X\\), con \\(p_k=P(X=k)\\). Si \\(X\\) est√° acotada, \\(M_X\\) est√° bien definida para todo \\(t\\) real; en cambio, si \\(X\\) no est√° acotada, es posible que el dominio de \\(M_X\\) no sea el conjunto de todos los reales. En todo caso, \\(\\phi\\) siempre est√° definida en cero, y \\(M(0) = 1\\).\nEs posible demostrar que si la f.g.m. de la v.a. \\(X\\) existe en un entorno de 0, entonces para todo \\(k &gt; 0\\),\n\\(\\mathbb{E}[|X|^k] &lt; \\infty.\\)\nM√°s a√∫n, la serie\n\\(M_X(t) =\n\\mathbb{E}(e^{tX})\n= \\mathbb{E}\\left(1 + \\sum_{k=1}^{\\infty} \\frac{t^k X^k}{k!}\\right)\n= 1 + \\sum_{n=1}^{\\infty} \\frac{t^k}{k!} \\mathbb{E}(X^k)\n\\tag{5.1}\\)\nes convergente y se puede derivar t√©rmino a t√©rmino. Obtenemos\n\\(M'_X(0) = \\mathbb{E}(X); \\quad M''_X(0) = \\mathbb{E}(X^2)\\)\ny en general\n\\(M_X^{(k)}(0) = \\mathbb{E}(X^k).\\)\nEs por esta √∫ltima propiedad que esta funci√≥n se conoce como funci√≥n generadora de momentos (f.g.m.).\nüé≤ Ejemplo: f.g.m. de la distribuci√≥n Binomial\nSea \\(X \\sim \\text{Binomial}(n, p)\\), es decir, la suma de \\(n\\) ensayos de Bernoulli con probabilidad de √©xito \\(p\\). La funci√≥n generadora de momentos es: Ejemplo fgm binomial\n\\(M_X(t) = \\mathbb{E}[e^{tX}] = (1 - p + p e^t)^n\\)\n ```r #\nmgf_binomial &lt;- function(t, n = 10, p = 0.3) {\n  (1 - p + p * exp(t))^n\n}\n\nt_vals &lt;- seq(-3, 3, length.out = 300)\nmgf_vals &lt;- sapply(t_vals, mgf_binomial)\n\nplot(t_vals, mgf_vals, type = \"l\", lwd = 2,\n     main = expression(\"F.G.M. para X ~ Binomial(10, 0.3)\"),\n     xlab = \"t\", ylab = expression(M[X](t)))\ngrid()\n``` \nüìà Ejemplo: f.g.m. de la distribuci√≥n Normal Est√°ndar\nSea \\(X \\sim \\mathcal{N}(0, 1)\\). Su funci√≥n generadora de momentos es:\n\\(M_X(t) = \\mathbb{E}[e^{tX}] = e^{\\frac{t^2}{2}}\\)\nEsta expresi√≥n se obtiene usando la forma cerrada del momento de una normal est√°ndar.\n ```r #\nmgf_normal &lt;- function(t) {\n  exp(t^2 / 2)\n}\n\nt_vals &lt;- seq(-3, 3, length.out = 300)\nmgf_vals &lt;- sapply(t_vals, mgf_normal)\n\nplot(t_vals, mgf_vals, type = \"l\", lwd = 2,\n     main = expression(\"F.G.M. para X ~ N(0, 1)\"),\n     xlab = \"t\", ylab = expression(M[X](t)))\ngrid()\n``` \n‚ùì Preguntas gu√≠a sobre la gr√°fica de la funci√≥n generadora de momentos\nüìå ¬øQu√© representa la gr√°fica de la f.g.m. \\(M_X(t)\\)?\nLa gr√°fica muestra c√≥mo evoluciona el valor esperado de \\(e^{tX}\\) cuando \\(t\\) var√≠a. Esta funci√≥n codifica todos los momentos de la variable aleatoria \\(X\\), y por tanto, contiene informaci√≥n completa sobre su distribuci√≥n (si existe un entorno donde la f.g.m. es finita).\n\nüß≠ ¬øQu√© se observa en la f.g.m. de una distribuci√≥n Binomial? \n![Gr√°fica MGF Binomial]\n#Preguntas y respuestas\n\n¬øC√≥mo es el comportamiento de la f.g.m. cerca de \\(t = 0\\)?\nEn \\(t = 0\\), siempre se cumple que \\(M_X(0) = 1\\), ya que:\n\\(M_X(0) = \\mathbb{E}[e^{0 \\cdot X}] = \\mathbb{E}[1] = 1\\)\n¬øQu√© indica la curvatura de la gr√°fica?\nLa curvatura refleja el crecimiento exponencial de los momentos. Si la curva crece r√°pidamente hacia la derecha, significa que los momentos (media, varianza, etc.) tambi√©n crecen con rapidez.\n¬øPor qu√© la gr√°fica es convexa?\nTodas las funciones generadoras de momentos son estrictamente convexas en el intervalo donde est√°n definidas. Esto es una consecuencia de que derivadas sucesivas representan momentos positivos.\n¬øQu√© pasa si cambio los par√°metros \\(n\\) y \\(p\\)?\nAumentar $ n $ o \\(p\\) tiende a elevar la f.g.m. en el lado derecho, reflejando una mayor media y varianza.\n\n\nüìà ¬øC√≥mo se comporta la f.g.m. para la Normal Est√°ndar? \n![Gr√°fica MGF Normal]\nPreguntas y respuestas\n\n¬øPor qu√© es sim√©trica respecto al eje $ t = 0 $?\nPorque la normal est√°ndar es sim√©trica alrededor de su media $ = 0 $, y su f.g.m. tiene la forma:\n\n\\(M_X(t) = e^{t^2 / 2}\\)\nlo cual es una funci√≥n par: \\(M_X(-t)= M_X(t)\\).\n\n¬øQu√© tan r√°pido crece la funci√≥n?\nMuy r√°pido. El crecimiento es exponencial cuadr√°tico. Esto implica que los momentos de la normal crecen r√°pidamente en magnitud.\n¬øC√≥mo se relaciona esta gr√°fica con los momentos de la normal?\nDerivando sucesivamente la f.g.m. en $ t = 0 $, se obtiene:\n\\(\\mathbb{E}[X^k] = M_X^{(k)}(0)\\)\nPor tanto, la gr√°fica ‚Äúencierra‚Äù toda la informaci√≥n sobre los momentos.\n\n\nüß† Conclusi√≥n\nEstas gr√°ficas te permiten visualizar la informaci√≥n estad√≠stica codificada en una variable aleatoria. La f.g.m. no es solo una herramienta algebraica para obtener momentos, sino una forma poderosa de describir el comportamiento global de la variable.\n\n¬øQu√© pasa si dos variables tienen la misma f.g.m.?\n¬°Tienen la misma distribuci√≥n! (si la f.g.m. est√° definida en un entorno de 0).\n\nEjemplo: Distribuci√≥n uniforme \\(U(a,b)\\)\nSi \\[X \\sim U(a,b),\\]\nsu densidad es\n\\[f(x) = \\frac{1}{b - a}\\quad\\text{para }a &lt; x &lt; b,\\]\ny su funci√≥n generadora de momentos viene dada por\n\n\n\n\n\\[M(t)= \\int_a^b \\frac{e^{t x}}{b - a}\\,dx= \\frac{e^{b t} - e^{a t}}{t\\,(b - a)}.\\]\n\n\n\n(5.2)\n\n\n\nEn el caso particular de la distribuci√≥n uniforme en \\((0,1)\\) se obtiene\n\\[M(t) = \\frac{e^t - 1}{t}.\\]\n\nPara derivar la f√≥rmula #(5.2) y obtener los momentos, podemos usar el desarrollo en serie de la funci√≥n exponencial:\n\\[M(t)= \\frac{1}{t\\,(b - a)}\\bigl(e^{b t} - e^{a t}\\bigr) \\\\\n= \\frac{1}{t\\,(b - a)}\\Bigl[\\bigl(1 + \\sum_{n=1}^\\infty \\tfrac{(b t)^n}{n!}\\bigr)\n                    -\\bigl(1 + \\sum_{n=1}^\\infty \\tfrac{(a t)^n}{n!}\\bigr)\\Bigr] \\\\\n= \\frac{1}{b - a}\\sum_{n=1}^\\infty \\frac{b^n - a^n}{n!}\\,t^{n-1}.\n\\]\nEste es el desarrollo de Maclaurin de \\(M(t)\\) en \\(t=0\\); por tanto, sus derivadas en cero satisfacen\n\n\n\n\\[M^{(k)}(0)= \\frac{b^{k+1} - a^{k+1}}{(k+1)\\,(b - a)}.\\]\n\n\n(5.3)\n\n\n\nEn particular:\n\n\\[M'(0)= \\frac{b^2 - a^2}{2\\,(b - a)}= \\frac{a + b}{2},\\] que coincide con \\(\\mathbb{E}(X)\\).\n\\[M''(0)= \\frac{b^3 - a^3}{3\\,(b - a)}= \\frac{a^2 + a b + b^2}{3},\\]\n\ny un c√°lculo directo muestra que la varianza es\n\\[\\mathrm{Var}(X)= \\mathbb{E}(X^2) - \\bigl(\\mathbb{E}(X)\\bigr)^2= \\frac{(a + b)^2}{12}.\\]\nObservaci√≥n importante Sea \\(X\\) una v.a. con f.g.m. \\(M_X\\) y sea \\(Y=aX+b\\) una transformaci√≥n lineal de \\(X\\), entonces\n\\[M_Y(t)=E(e^{tY})=E(e^{t(aX+b)})=E(e^{taX}e^{tb})=e^{tb}E(e^{taX})=e^{tb}M_X(at)\\]\nTeorema (fgm de suma de v.a.s)\nSi \\(X\\) tiene funci√≥n generadora de momentos \\(M(t)\\) que est√° definida en un entorno \\((-a,a)\\) de 0, entonces \\(M(t)\\) caracteriza a la distribuci√≥n de \\(X\\); es decir, si otra variable \\(Y\\) tiene la misma funci√≥n generadora de momentos, las distribuciones de \\(X\\) e \\(Y\\) coinciden.\n\nSi \\(X,Y\\) son variables aleatorias con funciones generadoras de momentos respectivas \\(M_X\\) y \\(M_Y\\) que existen en un dominio com√∫n \\(|t| &lt; d\\), entonces la f.g.m. de la suma \\(X+Y\\) est√° dada por \n\n\n\n\\[M_{X+Y}(t)= \\mathbb{E}\\bigl[e^{t(X+Y)}\\bigr]= \\mathbb{E}\\bigl[e^{tX}\\,e^{tY}\\bigr]=\\mathbb{E}\\bigl[e^{tX}\\bigr]\\;\\mathbb{E}\\bigl[e^{tY}\\bigr]= M_X(t)\\,M_Y(t).\n\\tag{5.5}\n\\]\n\n\n(5.5)\n\n\n\nEste resultado se extiende a la suma de \\(n\\) variables aleatorias independientes. Si\n\\[S_n = X_1 + \\cdots + X_n,\\]\nentonces\n\\[M_{S_n}(t)= \\mathbb{E}\\bigl[e^{tS_n}\\bigr]= \\mathbb{E}\\Bigl[e^{t\\sum_{i=1}^n X_i}\\Bigr]= \\prod_{i=1}^n\\mathbb{E}\\bigl[e^{tX_i}\\bigr]= \\prod_{i=1}^n M_{X_i}(t).\\]\nLa funci√≥n generadora de momentos resulta particularmente √∫til cuando consideramos sucesiones de variables aleatorias, como lo muestra el siguiente teorema que enunciamos sin demostraci√≥n:\n\nTeorema (de Continuidad)\nSea \\(F_n(x)\\), \\(n\\ge1\\), una sucesi√≥n de funciones de distribuci√≥n con funciones generadoras de momentos respectivas \\(M_n(t)\\), definidas en \\(|t|&lt;b\\). Supongamos que cuando \\(n\\to\\infty\\),\n\\[\nM_n(t)\\,\\longrightarrow\\,M(t)\n\\quad\\text{para }|t|\\le a,\n\\]\ndonde \\(M(t)\\) es la funci√≥n generadora de momentos de la distribuci√≥n l√≠mite \\(F(x)\\). Entonces\n\\[\nF_n(x)\\,\\longrightarrow\\,F(x)\n\\quad\\text{cuando }n\\to\\infty\n\\]\npara todo punto \\(x\\) en el cual \\(F\\) es continua.\nTeorema de Laplace‚ÄìMoivre\nSea \\(X_1, X_2, \\ldots, X_n\\) una sucesi√≥n de variables aleatorias i.i.d. con distribuci√≥n ( (p) ), donde ( 0 &lt; p &lt; 1 ). Sea:\n\\[\nS_n = X_1 + X_2 + \\cdots + X_n \\sim \\text{Binomial}(n, p)\n\\]\ny consideremos la variable tipificada:\n\\[\nZ_n = \\frac{S_n - np}{\\sqrt{np(1 - p)}}\n\\]\nEntonces, cuando ( n ), se tiene convergencia en distribuci√≥n a una normal est√°ndar:\n\\[\nZ_n \\xrightarrow{d} \\mathcal{N}(0, 1)\n\\]\nes decir,\n\\[\n\\lim_{n \\to \\infty} \\mathbb{P}(Z_n \\leq z) = \\Phi(z), \\quad \\text{para todo } z \\in \\mathbb{R}\n\\]\ndonde ( (z) ) es la funci√≥n de distribuci√≥n acumulada de la normal est√°ndar.\n\nDemostraci√≥n usando funciones generadoras de momentos\nLa funci√≥n generadora de momentos (mgf) de \\(S_n \\sim \\text{Binomial}(n, p)\\) es:\n\\[\nM_{S_n}(t) = \\left(1 - p + p e^t\\right)^n\n\\]\nQueremos obtener la mgf de la variable tipificada ( Z_n ). Usamos la propiedad de cambio de variable de la mgf:\n\\[\nM_{Z_n}(t) = \\mathbb{E}\\left[ e^{t Z_n} \\right]\n= \\mathbb{E}\\left[ e^{t \\cdot \\frac{S_n - np}{\\sqrt{np(1 - p)}}} \\right]\n= e^{-t \\cdot \\frac{np}{\\sqrt{np(1 - p)}}} \\cdot M_{S_n}\\left( \\frac{t}{\\sqrt{np(1 - p)}} \\right)\n\\]\nSustituimos la mgf de ( S_n ):\n\\[\nM_{Z_n}(t) = \\exp\\left( -t \\cdot \\frac{np}{\\sqrt{np(1 - p)}} \\right)\n\\cdot \\left( 1 - p + p e^{t / \\sqrt{np(1 - p)}} \\right)^n\n\\]\n\nAproximaci√≥n por series de Taylor\nExpandimos \\(e^{t / \\sqrt{np(1 - p)}}\\) para \\(n\\) grande:\n\\[\ne^{t / \\sqrt{np(1 - p)}} = 1 + \\frac{t}{\\sqrt{np(1 - p)}} + \\frac{t^2}{2np(1 - p)} + \\cdots\n\\]\nEntonces:\n\\[\n1 - p + p e^{t / \\sqrt{np(1 - p)}} \\approx 1 + \\frac{pt}{\\sqrt{np(1 - p)}} + \\frac{pt^2}{2np(1 - p)} + \\cdots\n\\]\nUsamos que \\(\\log(1 + x) \\approx x - \\frac{x^2}{2} + \\cdots\\) para \\(x \\approx 0\\):\n\\[\\log M_{Z_n}(t) \\approx -t \\cdot \\frac{np}{\\sqrt{np(1 - p)}}+ n \\left( \\frac{pt}{\\sqrt{np(1 - p)}} + \\frac{pt^2}{2np(1 - p)} \\right)\\]\nSimplificamos:\n\nEl t√©rmino lineal se cancela:\n\n\\[\n-t \\cdot \\frac{np}{\\sqrt{np(1 - p)}} + n \\cdot \\frac{pt}{\\sqrt{np(1 - p)}} = 0\n\\]\n\nQueda:\n\n\\[\n\\log M_{Z_n}(t) \\to \\frac{t^2}{2}, \\quad \\text{cuando } n \\to \\infty\n\\]\nPor tanto:\n\\[\nM_{Z_n}(t) \\to e^{t^2 / 2}\n\\]\n\nConclusi√≥n\nComo \\(e^{t^2/2}\\) es la mgf de \\(\\mathcal{N}(0, 1)\\), y por el teorema de unicidad de la funci√≥n generadora de momentos:\n\\[\nZ_n \\xrightarrow{d} \\mathcal{N}(0, 1)\n\\]\nEsto concluye la demostraci√≥n del Teorema de Laplace‚ÄìMoivre utilizando funciones generadoras de momentos.\n\n\n4.0.0.5 1.2.5 Muestra aleatoria simple\nSea \\(\\underset{\\sim}{X} =(X_1 ,..., X_n)\\) un vector aleatorio. Se dice que sus componentes \\(X_1 ,..., X_n\\) son si \\(P(X_1\\leq x_1 ,..., X_n\\leq x_n)=P(X_1\\leq x_1)...P(X_n\\leq x_n)\\) para cualesquiera valores \\(x_1,..., x_n\\) .\nSi adem√°s la distribuci√≥n de las \\(n\\) variables aleatorias \\(X_i\\) es la misma, se dice que \\(X_1 ,...,X_n\\) son variables aleatorias independientes e id√©nticamente distribuidas, o bien que son v.a.i.i.d o simplemente i.i.d.\nSi \\(\\underset{\\sim}{X} =(X_1 ,..., X_n)\\) y \\(X_1 ,..., X_n\\) son i.i.d. con funci√≥n de densidad (en su caso, de masa) \\(f_X\\) , la distribuci√≥n conjunta de \\(\\underset{\\sim}{X}\\) viene dada por la funci√≥n de densidad (en su caso, de masa) conjunta \\[\n\\begin{align*}\nf_{\\underset{\\sim}{X}}(\\underset{\\sim}{x})&=f_{(X_1 ,..., X_n)}(x_1 ,..., x_n)\\\\\n&=f_{(X_1)}(x_1)...f_{(X_n)}(x_n)\\\\\n&=\\prod_{i=1}^{n}f_{(X_i)}(x_i)\n\\end{align*}\n\\]\nA un vector \\(\\underset{\\sim}{X} =(X_1 ,..., X_n)\\) de v.a.i.i.d. con distribuci√≥n igual a la de la variable aleatoria \\(X\\) se le denomina tambi√©n muestra aleatoria simple de \\(X\\) (m.a.s de \\(X\\)).\nEsto responde al hecho siguiente. Supongamos que se desea estudiar la caracter√≠stica \\(X\\) de los individuos de una poblaci√≥n de tama√±o infinito. Definimos el experimento consistente en elegir aleatoriamente un individuo de la poblaci√≥n y llamamos \\(X\\) al valor de la caracter√≠stica de inter√©s en ese individuo. X es una variable aleatoria.\nSi definimos un nuevo experimento consistente en elegir una muestra aleatoria de n individuos y se anota \\(X_i\\), el valor de la caracter√≠stica en el individuo i-√©simo, entonces X \\(=(X_1 ,..., X_n)\\) es una colecci√≥n de n v.a.i.i.d. con distribuci√≥n igual a la de la variable aleatoria \\(X\\), es decir, \\(X_1 ,..., X_n\\) es una m.a.s. de X.\n\n\n4.0.0.6 1.2.6 Modelo param√©trico\nUsualmente la ley de probabilidad de una variable aleatoria se supone perteneciente a un modelo matem√°tico que depende s√≥lo de un n√∫mero finito de par√°metros: \\(f_X \\in\\{f(x|\\theta):\\theta \\in \\Theta \\subseteq \\mathbb{R}^k\\}\\). Escribiremos alternativamente \\(f(x;\\theta)\\), \\(f(x|\\theta)\\) o \\(f_\\theta(x)\\).\nDefinici√≥n El conjunto de distribuciones dadas por \\(f_\\theta(x)\\), \\(\\theta \\in \\Theta\\) se llama familia param√©trica de distribuciones. \\(\\Theta\\) es el conjunto de par√°metros.\nDefinici√≥n La correspondiente distribuci√≥n conjunta de una muestra aleatoria simple de \\(X\\) viene dada por la funci√≥n de densidad (o funci√≥n de masa de probabilidad, seg√∫n el caso)\n\\[\nf_{X_{\\sim}}(x_{\\sim} \\mid \\theta) = \\prod_{i=1}^{n} f_{\\theta}(x_i)\n\\]\nA esta funci√≥n la llamaremos funci√≥n de verosimilitud de la muestra \\(X_{\\sim}\\). Utilizaremos este t√©rmino para referirnos indistintamente a la funci√≥n de densidad conjunta (si las variables aleatorias son continuas) o a la funci√≥n de masa conjunta (si son discretas).\n\n\n4.0.0.7 1.2.7 Sumas de variables aleatorias\nCuando se obtiene una muestra aleatoria simple \\(X_{1},X_{2},\\ldots,X_{n}\\) normalmente se calculan a partir de ellas cantidades que resumen los valores observados. Cualquiera de estos res√∫menes se puede expresar como una funci√≥n \\(T(x_1,\\ldots,x_n)\\) definida en el espacio \\(\\mathcal{X}^n\\subseteq\\mathbb{R}^n\\) donde est√°n las im√°genes del vector \\((X_{1},X_{2},\\ldots,X_{n})\\).\nEsta funci√≥n \\(T\\) puede devolver valores de \\(\\mathbb{R}\\), \\(\\mathbb{R}^2\\) o, en general, \\(\\mathbb{R}^k\\).\n\\[T(X_1 , \\ldots, X_n)=\\sum_{i=1}^{n}X_i,\\bar{X},\\bar{X}+3, \\min{X_1 , \\ldots, X_n},\\] \\[T(X_1 , \\ldots, X_n)=\\left(\\sum_{i=1}^{n}X_i,\\sum_{i=1}^{n}(X_i-\\bar{X})^2\\right),\\] \\[T(X_1 , \\ldots, X_n)=\\left(\\min\\{X_1 , \\ldots, X_n\\},\\sum_{i=1}^{n}X_i,\\sum_{i=1}^{n}(X_i-\\bar{X})^2\\right),\\] \\[T(X_1 , \\ldots, X_n)= (X_1 , \\ldots, X_n)\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Variable aleatoria</span>"
    ]
  },
  {
    "objectID": "index.html#c√≥mo-navegar-este-libro",
    "href": "index.html#c√≥mo-navegar-este-libro",
    "title": "Inferencia Estad√≠stica",
    "section": "1.1 ¬øC√≥mo navegar este libro?",
    "text": "1.1 ¬øC√≥mo navegar este libro?\n\nUsa el √≠ndice lateral izquierdo para acceder a cada cap√≠tulo y subcap√≠tulo.\nHaz uso del buscador para encontrar conceptos o t√©rminos clave.\nRevisa los apartados de ‚ÄúLista de problemas‚Äù incluidos al final de cada secci√≥n para practicar.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Inferencia Estad√≠stica</span>"
    ]
  },
  {
    "objectID": "index.html#bienvenidao",
    "href": "index.html#bienvenidao",
    "title": "Inferencia Estad√≠stica",
    "section": "1.2 ¬°Bienvenida/o!",
    "text": "1.2 ¬°Bienvenida/o!\nTe invito a recorrer este texto con atenci√≥n, curiosidad y sentido cr√≠tico.\nEspero que este libro te acompa√±e, rete y apoye en tu formaci√≥n como profesional en ciencias de datos o √°reas relacionadas.\n\nDatos y modelos\n\nVariable aleatoria",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Inferencia Estad√≠stica</span>"
    ]
  }
]